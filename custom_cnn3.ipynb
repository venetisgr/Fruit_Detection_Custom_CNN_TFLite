{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EDG9SjhsTgjE"},"outputs":[],"source":["#datasets from:\n","#https://www.kaggle.com/datasets/moltean/fruits"]},{"cell_type":"markdown","metadata":{"id":"uN6C5uhzTcav"},"source":["## Library load"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28833,"status":"ok","timestamp":1656690663009,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"uVa1ZItwTZak","outputId":"91d1572d-dca8-48c7-c934-a5d418b54cb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1656690663010,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"vlHPTBNBWiTD","outputId":"20e7cc24-3674-40bb-9d8b-7f476f2de4c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Portofolio/fruit_quant_aware\n"]}],"source":["cd /content/drive/MyDrive/Portofolio/fruit_quant_aware"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":742,"status":"ok","timestamp":1656690663748,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"6QH4Yz7dTZ28","outputId":"3f5b1584-56d8-478c-b660-5018dbee8b16"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Callbacks\t\t    pruning.ipynb\t Training\n"," cluster_saved_models\t    quant_saved_models\t Validation\n","'Copy of mobilenet.ipynb'   saved_models\t weight_clustering.ipynb\n"," custom_cnn3.ipynb\t    Test\n"," pruned_saved_models\t    TFLite_Models\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5700,"status":"ok","timestamp":1656690669443,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"ksqxnIA5LlAj","outputId":"a6117a89-163c-47fa-9496-3de9f3ddc60a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-model-optimization\n","  Downloading tensorflow_model_optimization-0.7.2-py2.py3-none-any.whl (237 kB)\n","\u001b[K     |████████████████████████████████| 237 kB 15.3 MB/s \n","\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization) (0.1.7)\n","Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization) (1.15.0)\n","Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization) (1.21.6)\n","Installing collected packages: tensorflow-model-optimization\n","Successfully installed tensorflow-model-optimization-0.7.2\n"]}],"source":[" !pip install  tensorflow-model-optimization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8xDcJH5TZ8A"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import os\n","import glob\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","from PIL import Image\n","from matplotlib import image as plt_image\n","import cv2\n","\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input \n","#mobilenet expects inputs in the range [-1 1] of float data type\n","\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n","from tensorflow.keras import Sequential \n","#https://keras.io/api/applications/mobilenet/ #mobilenet explanation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLROIApITZ9u"},"outputs":[],"source":["import tensorflow_model_optimization as tfmot\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"um-SAl-qTaA4"},"outputs":[],"source":["np.random.seed(42)# keras seed fixing \n","tf.random.set_seed(42)# tensorflow seed fixing"]},{"cell_type":"code","source":["print(tf.__version__) #2.8.2\n","print(tf.keras.__version__) #2.8.0\n","print(tfmot.__version__) #0.7.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTsLT2h1hnsJ","executionInfo":{"status":"ok","timestamp":1656693658320,"user_tz":240,"elapsed":205,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"1598fbd6-e207-4b95-fa04-cd3c8e757900"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.2\n","2.8.0\n","0.7.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"lx3VD0bqZ8fX"},"source":["## Dataset Information"]},{"cell_type":"markdown","metadata":{"id":"f4COuq8MWdPa"},"source":["### train data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12001,"status":"ok","timestamp":1656694458156,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"BVaZlcxhTaEV","outputId":"85f90115-f7da-468b-d7bd-ddf0a6f1143e"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of images in total 6231\n"]}],"source":["print(\"number of images in total\", len(glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\")))"]},{"cell_type":"markdown","source":["### val data"],"metadata":{"id":"NrhTJxd2Si9V"}},{"cell_type":"code","source":["print(\"number of images in total\", len(glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Validation/*/*\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnN86ZQ5SnKU","executionInfo":{"status":"ok","timestamp":1656694464140,"user_tz":240,"elapsed":5987,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"11c23409-d407-4b95-f577-62594b85413c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of images in total 3114\n"]}]},{"cell_type":"markdown","metadata":{"id":"XgrPoXgfWf3o"},"source":["### test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5781,"status":"ok","timestamp":1656694469918,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"70ioTQa_Wf-k","outputId":"edfbdb65-16fe-49c8-9039-729c8e819aaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of images in total 3110\n"]}],"source":["print(\"number of images in total\", len(glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Test/*/*\")))"]},{"cell_type":"markdown","metadata":{"id":"HjagyiZlgr4i"},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1656694469918,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"zRFWKaijh_05","outputId":"4c4df4e2-008b-4370-e08c-00e7d2daa744"},"outputs":[{"output_type":"stream","name":"stdout","text":["(48, 48) (48, 48, 3)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.001"]},"metadata":{},"execution_count":24}],"source":["class hyperparams:\n","  def __init__(self):\n","    self.dim2d = (48,48) #image dimensions we want downscale\n","    self.dim3d = (48,48,3) #128 is the minimum for mobilenet\n","    self.batch_size = 64\n","    self.no_epochs = 50#30\n","    self.lr = 1e-3\n","  \n","hparams =  hyperparams()\n","print(hparams.dim2d,hparams.dim3d)\n","hparams.lr"]},{"cell_type":"markdown","metadata":{"id":"UftMvxqmZ-_g"},"source":["##Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"YhIrBKioc7vB"},"source":["### augmentation and preprocess\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vxgBmT_T_aZ"},"outputs":[],"source":["def preproc(inp):#custom one without using tf functions\n","  return (inp*1.0/255)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"riRziuW9TaH_"},"outputs":[],"source":["train_datagen = ImageDataGenerator(#featurewise_center=True,\n","                             rotation_range=(0-30),\n","                             width_shift_range=0.2,\n","                             height_shift_range=0.2,\n","                             brightness_range=[0.5,1.5],\n","                             shear_range=0.2, \n","                             zoom_range=0.2,\n","                             channel_shift_range=0.2,\n","                             horizontal_flip=True, \n","                             #vertical_flip=True,\n","                             fill_mode='nearest',\n","                             preprocessing_function=preproc,\n","                             \n","                             dtype=float)\n","\n","val_datagen = ImageDataGenerator(\n","                                  dtype=float,\n","                                  preprocessing_function=preproc\n","                                  ) #no augmentation for test \n","\n","\n","test_datagen = ImageDataGenerator(\n","                                  dtype=float,\n","                                  preprocessing_function=preproc\n","                                  ) #no augmentation for test \n"]},{"cell_type":"markdown","metadata":{"id":"AdWIx7H4dAE0"},"source":["### post augmentation images and generator creation "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":697,"status":"ok","timestamp":1656694470610,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"H_G-rgV1gizy","outputId":"8471f997-d7f6-4745-846c-484067ed81eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 6231 images belonging to 24 classes.\n","Found 3114 images belonging to 24 classes.\n","Found 3110 images belonging to 24 classes.\n"]}],"source":["train_generator = train_datagen.flow_from_directory(\n","    \"Training\",\n","    target_size=hparams.dim2d,\n","    batch_size=hparams.batch_size,\n","    class_mode='categorical',\n","    shuffle=True,\n","    color_mode=\"rgb\",\n","    interpolation=\"bilinear\",\n","    ) # set as training data\n","\n","validation_generator = val_datagen.flow_from_directory(\n","    \"Validation\", # same directory as training data\n","    target_size=hparams.dim2d,\n","    batch_size=hparams.batch_size,\n","    class_mode='categorical',\n","    shuffle=False,\n","    color_mode=\"rgb\",\n","    interpolation=\"bilinear\",\n","    ) # set as validation data\n","\n","\n","test_generator = test_datagen.flow_from_directory(\n","    \"Test\", \n","    target_size=hparams.dim2d,\n","    batch_size=hparams.batch_size,\n","    class_mode='categorical',\n","    interpolation=\"bilinear\",\n","    color_mode=\"rgb\",\n","    ) # set as test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34729,"status":"ok","timestamp":1656694505337,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"VTFQxnAqO25z","outputId":"6b23ed4c-6e78-4714-c8de-17ab72c6333a"},"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 48, 48, 3)\n","float32\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.0, 1.0)"]},"metadata":{},"execution_count":28}],"source":[" img = next(train_generator)[0]\n"," print(img.shape)\n"," print(img.dtype)\n"," img.min(),img.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32926,"status":"ok","timestamp":1656694538259,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"92If51GmPH4G","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b142a7b9-6029-41a8-cb12-902d246139c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 48, 48, 3)\n","float32\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.0, 1.0)"]},"metadata":{},"execution_count":29}],"source":[" img = next(validation_generator)[0]\n"," print(img.shape)\n"," print(img.dtype)\n"," img.min(),img.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":31830,"status":"ok","timestamp":1656694570085,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"atWlkdNOPO0U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8f351ea4-20b1-468c-90fe-e804f0377208"},"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 48, 48, 3)\n","float32\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.0, 1.0)"]},"metadata":{},"execution_count":30}],"source":[" img = next(test_generator)[0]\n"," print(img.shape)\n"," print(img.dtype)\n"," img.min(),img.max()"]},{"cell_type":"markdown","metadata":{"id":"aXvkchFoWrDX"},"source":["## Model creation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":227,"status":"ok","timestamp":1656461702446,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"Jd5f0pvUYu2A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1a7c1392-5839-4d5c-f2da-37448b2f727e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_6 (Conv2D)           (None, 46, 46, 16)        448       \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 46, 46, 16)       64        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 44, 44, 16)        2320      \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 44, 44, 16)       64        \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 22, 22, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_3 (Dropout)         (None, 22, 22, 16)        0         \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 20, 20, 32)        4640      \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 20, 20, 32)       128       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 18, 18, 32)        9248      \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 18, 18, 32)       128       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 9, 9, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_4 (Dropout)         (None, 9, 9, 32)          0         \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 7, 7, 64)          18496     \n","                                                                 \n"," batch_normalization_10 (Bat  (None, 7, 7, 64)         256       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 5, 5, 64)          36928     \n","                                                                 \n"," batch_normalization_11 (Bat  (None, 5, 5, 64)         256       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 2, 2, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_5 (Dropout)         (None, 2, 2, 64)          0         \n","                                                                 \n"," global_average_pooling2d_1   (None, 64)               0         \n"," (GlobalAveragePooling2D)                                        \n","                                                                 \n"," dense_1 (Dense)             (None, 24)                1560      \n","                                                                 \n","=================================================================\n","Total params: 74,536\n","Trainable params: 74,088\n","Non-trainable params: 448\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n","\n","model = Sequential()\n","\n","model.add(Conv2D(16, (3, 3), activation='relu', input_shape=hparams.dim3d))\n","model.add(BatchNormalization())\n","model.add(Conv2D(16, (3, 3), activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.5))\n","\n","model.add(Conv2D(32, (3, 3), activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(32, (3, 3), activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.5))\n","\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.5))\n","\n","model.add(tf.keras.layers.GlobalAveragePooling2D())\n","model.add(Dense(24 , activation='softmax')) # 2 classes\n","\n","model.summary()"]},{"cell_type":"markdown","source":["**IMPORTANT**\n","\n","Each 32float param will be converted to an 8bit int one. Thus, for the QUANTIZED MODEL we will need roughly 72.3KB/0.07 MB of memory (74,088/(1024^2)) for the weights. The quantized tflite model will need more space though in order to store the architecture information and the quantization parameters that are needed."],"metadata":{"id":"Xjhc3k55wc42"}},{"cell_type":"markdown","metadata":{"id":"lrVIgCDnsS3-"},"source":["## Callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yeeyuy9hY4vj"},"outputs":[],"source":["# https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_AkZVFOdvg3"},"outputs":[],"source":["'''\n","Adding Callbacks and EarlyStopping\n","Callbacks and Checkpoints help to keep an eye on model while training and stop the training\n","if the performance has reached an optimum.\n","'''\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","filepath = 'Callbacks/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5'\n","checkpoint = ModelCheckpoint(filepath, monitor = 'val_accuracy', \n","                             verbose = 1,\n","                             save_best_only = True,\n","                             mode = 'max',\n","                             save_freq = \"epoch\", #check and save at the end of the epoch   \n","                             save_weights_only=False,   #save model too   \n","                             )#best accuracy saved\n","\n","early_stop = EarlyStopping(monitor = 'val_loss',\n","                           patience = 7, #wait 7 epochs before you restore best weights and stop model trainng\n","                           mode=\"min\", \n","                           verbose = 1,\n","                           min_delta=0.01,\n","                           restore_best_weights=True)#go to the model that had the best accuracy before the early stopping before patience epochs\n","\n","#https://keras.io/api/callbacks/model_checkpoint/\n","#https://keras.io/api/callbacks/early_stopping/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FE8mYSKMgRpP"},"source":["## Learning Rate and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BrdXrbf3j_Un"},"outputs":[],"source":["#https://towardsdatascience.com/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c\n","#time decay\n","from tensorflow.keras.optimizers import Adam\n","\n","\n","\n","def lr_time_based_decay(epoch, lr):\n","    initial_learning_rate = hparams.lr \n","    epochs = hparams.no_epochs\n","    decay = initial_learning_rate / (epochs) *1000\n","\n","    return lr * 1 / (1 + decay * epoch)\n","\n","time_decay_learning_rate = tf.keras.callbacks.LearningRateScheduler (lr_time_based_decay, verbose=1) #CALLBACK \n","callbacks = [checkpoint, early_stop,time_decay_learning_rate]\n","optimizer = Adam(learning_rate=hparams.lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fITSG7Uj_gQ"},"outputs":[],"source":["# #https://towardsdatascience.com/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c\n","# #step decay\n","# import math\n","# from tensorflow.keras.optimizers import Adam\n","\n","\n","\n","# def lr_step_decay(epoch, lr):\n","#     initial_learning_rate = hparams.lr #0.1\n","#     drop_rate = 0.5 #halfs the learning rate\n","#     epochs_drop = 10.0\n","#     return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n","\n","# step_decay_learning_rate = tensorflow.keras.callbacks.LearningRateScheduler (lr_step_decay, verbose=1) #CALLBACK \n","# callbacks = [model_checkpoint_callback,model_earlystop_callback,step_decay_learning_rate]\n","# optimizer = Adam(learning_rate=hparams.lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5JTIUSNtPTw"},"outputs":[],"source":["# #no policies\n","\n","# #optimizer \n","\n","# from tensorflow.keras.optimizers import Adam\n","# callbacks = [checkpoint, early_stop]\n","# optimizer = Adam(learning_rate=hparams.lr)"]},{"cell_type":"markdown","metadata":{"id":"DTG9gwi1eeT2"},"source":["## Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_pVmi9ZFVtW"},"outputs":[],"source":["model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics = 'accuracy')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1388032,"status":"ok","timestamp":1656463093073,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"oDZ_RtTBeej9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4dd6849a-2d28-4440-cf6b-48c18807e293"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n","Epoch 1/50\n","97/97 [==============================] - ETA: 0s - loss: 2.3460 - accuracy: 0.2538\n","Epoch 1: val_accuracy improved from -inf to 0.05143, saving model to Callbacks/weights-improvement-01-0.05.hdf5\n","97/97 [==============================] - 55s 557ms/step - loss: 2.3460 - accuracy: 0.2538 - val_loss: 3.5117 - val_accuracy: 0.0514 - lr: 0.0010\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 0.0009803922034288739.\n","Epoch 2/50\n","97/97 [==============================] - ETA: 0s - loss: 1.4618 - accuracy: 0.4737\n","Epoch 2: val_accuracy improved from 0.05143 to 0.05436, saving model to Callbacks/weights-improvement-02-0.05.hdf5\n","97/97 [==============================] - 53s 549ms/step - loss: 1.4618 - accuracy: 0.4737 - val_loss: 4.9734 - val_accuracy: 0.0544 - lr: 9.8039e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 0.0009426848219635967.\n","Epoch 3/50\n","97/97 [==============================] - ETA: 0s - loss: 1.2230 - accuracy: 0.5567\n","Epoch 3: val_accuracy improved from 0.05436 to 0.07357, saving model to Callbacks/weights-improvement-03-0.07.hdf5\n","97/97 [==============================] - 54s 553ms/step - loss: 1.2230 - accuracy: 0.5567 - val_loss: 5.2228 - val_accuracy: 0.0736 - lr: 9.4268e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 0.0008893253079633105.\n","Epoch 4/50\n","97/97 [==============================] - ETA: 0s - loss: 1.0405 - accuracy: 0.6144\n","Epoch 4: val_accuracy improved from 0.07357 to 0.18197, saving model to Callbacks/weights-improvement-04-0.18.hdf5\n","97/97 [==============================] - 53s 551ms/step - loss: 1.0405 - accuracy: 0.6144 - val_loss: 3.5765 - val_accuracy: 0.1820 - lr: 8.8933e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 0.0008234493348195596.\n","Epoch 5/50\n","97/97 [==============================] - ETA: 0s - loss: 0.9583 - accuracy: 0.6507\n","Epoch 5: val_accuracy improved from 0.18197 to 0.30990, saving model to Callbacks/weights-improvement-05-0.31.hdf5\n","97/97 [==============================] - 53s 554ms/step - loss: 0.9583 - accuracy: 0.6507 - val_loss: 2.4107 - val_accuracy: 0.3099 - lr: 8.2345e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 0.0007485903122208334.\n","Epoch 6/50\n","97/97 [==============================] - ETA: 0s - loss: 0.8507 - accuracy: 0.6887\n","Epoch 6: val_accuracy improved from 0.30990 to 0.41960, saving model to Callbacks/weights-improvement-06-0.42.hdf5\n","97/97 [==============================] - 53s 549ms/step - loss: 0.8507 - accuracy: 0.6887 - val_loss: 2.0224 - val_accuracy: 0.4196 - lr: 7.4859e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 0.0006683842262386211.\n","Epoch 7/50\n","97/97 [==============================] - ETA: 0s - loss: 0.7643 - accuracy: 0.7179\n","Epoch 7: val_accuracy improved from 0.41960 to 0.68034, saving model to Callbacks/weights-improvement-07-0.68.hdf5\n","97/97 [==============================] - 53s 551ms/step - loss: 0.7643 - accuracy: 0.7179 - val_loss: 1.1482 - val_accuracy: 0.6803 - lr: 6.6838e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 0.0005863019747234749.\n","Epoch 8/50\n","97/97 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.7475\n","Epoch 8: val_accuracy improved from 0.68034 to 0.77409, saving model to Callbacks/weights-improvement-08-0.77.hdf5\n","97/97 [==============================] - 53s 551ms/step - loss: 0.6898 - accuracy: 0.7475 - val_loss: 0.9648 - val_accuracy: 0.7741 - lr: 5.8630e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 0.0005054327253862446.\n","Epoch 9/50\n","97/97 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.7574\n","Epoch 9: val_accuracy did not improve from 0.77409\n","97/97 [==============================] - 54s 555ms/step - loss: 0.6476 - accuracy: 0.7574 - val_loss: 1.1937 - val_accuracy: 0.6657 - lr: 5.0543e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 0.000428332813020985.\n","Epoch 10/50\n","97/97 [==============================] - ETA: 0s - loss: 0.6097 - accuracy: 0.7733\n","Epoch 10: val_accuracy improved from 0.77409 to 0.79492, saving model to Callbacks/weights-improvement-10-0.79.hdf5\n","97/97 [==============================] - 53s 552ms/step - loss: 0.6097 - accuracy: 0.7733 - val_loss: 0.8580 - val_accuracy: 0.7949 - lr: 4.2833e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 0.00035694400139618665.\n","Epoch 11/50\n","97/97 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.7858\n","Epoch 11: val_accuracy improved from 0.79492 to 0.83594, saving model to Callbacks/weights-improvement-11-0.84.hdf5\n","97/97 [==============================] - 53s 551ms/step - loss: 0.5784 - accuracy: 0.7858 - val_loss: 0.8284 - val_accuracy: 0.8359 - lr: 3.5694e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 0.0002925770383969438.\n","Epoch 12/50\n","97/97 [==============================] - ETA: 0s - loss: 0.5377 - accuracy: 0.7993\n","Epoch 12: val_accuracy did not improve from 0.83594\n","97/97 [==============================] - 53s 550ms/step - loss: 0.5377 - accuracy: 0.7993 - val_loss: 0.8875 - val_accuracy: 0.7676 - lr: 2.9258e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 0.0002359492129706327.\n","Epoch 13/50\n","97/97 [==============================] - ETA: 0s - loss: 0.5199 - accuracy: 0.8088\n","Epoch 13: val_accuracy improved from 0.83594 to 0.83757, saving model to Callbacks/weights-improvement-13-0.84.hdf5\n","97/97 [==============================] - 53s 549ms/step - loss: 0.5199 - accuracy: 0.8088 - val_loss: 0.7657 - val_accuracy: 0.8376 - lr: 2.3595e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 0.00018726127491968254.\n","Epoch 14/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.8174\n","Epoch 14: val_accuracy did not improve from 0.83757\n","97/97 [==============================] - 53s 550ms/step - loss: 0.4996 - accuracy: 0.8174 - val_loss: 0.8410 - val_accuracy: 0.7819 - lr: 1.8726e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 0.00014629787301601027.\n","Epoch 15/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4674 - accuracy: 0.8299\n","Epoch 15: val_accuracy improved from 0.83757 to 0.84115, saving model to Callbacks/weights-improvement-15-0.84.hdf5\n","97/97 [==============================] - 54s 554ms/step - loss: 0.4674 - accuracy: 0.8299 - val_loss: 0.7382 - val_accuracy: 0.8411 - lr: 1.4630e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 0.00011253682224868008.\n","Epoch 16/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4631 - accuracy: 0.8224\n","Epoch 16: val_accuracy improved from 0.84115 to 0.84245, saving model to Callbacks/weights-improvement-16-0.84.hdf5\n","97/97 [==============================] - 53s 550ms/step - loss: 0.4631 - accuracy: 0.8224 - val_loss: 0.7748 - val_accuracy: 0.8424 - lr: 1.1254e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 8.525516794620533e-05.\n","Epoch 17/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.8260\n","Epoch 17: val_accuracy did not improve from 0.84245\n","97/97 [==============================] - 53s 544ms/step - loss: 0.4675 - accuracy: 0.8260 - val_loss: 0.7940 - val_accuracy: 0.8353 - lr: 8.5255e-05\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 6.362325933226731e-05.\n","Epoch 18/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4476 - accuracy: 0.8302\n","Epoch 18: val_accuracy improved from 0.84245 to 0.85254, saving model to Callbacks/weights-improvement-18-0.85.hdf5\n","97/97 [==============================] - 53s 552ms/step - loss: 0.4476 - accuracy: 0.8302 - val_loss: 0.7658 - val_accuracy: 0.8525 - lr: 6.3623e-05\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 4.678180737434613e-05.\n","Epoch 19/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8424\n","Epoch 19: val_accuracy improved from 0.85254 to 0.86133, saving model to Callbacks/weights-improvement-19-0.86.hdf5\n","97/97 [==============================] - 53s 551ms/step - loss: 0.4213 - accuracy: 0.8424 - val_loss: 0.7239 - val_accuracy: 0.8613 - lr: 4.6782e-05\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 3.3899859640835046e-05.\n","Epoch 20/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4384 - accuracy: 0.8364\n","Epoch 20: val_accuracy did not improve from 0.86133\n","97/97 [==============================] - 53s 551ms/step - loss: 0.4384 - accuracy: 0.8364 - val_loss: 0.7370 - val_accuracy: 0.8467 - lr: 3.3900e-05\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 2.4214184252611763e-05.\n","Epoch 21/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.8429\n","Epoch 21: val_accuracy did not improve from 0.86133\n","97/97 [==============================] - 53s 551ms/step - loss: 0.4331 - accuracy: 0.8429 - val_loss: 0.7540 - val_accuracy: 0.8548 - lr: 2.4214e-05\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 1.7052242980407498e-05.\n","Epoch 22/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.8453\n","Epoch 22: val_accuracy did not improve from 0.86133\n","97/97 [==============================] - 53s 549ms/step - loss: 0.4220 - accuracy: 0.8453 - val_loss: 0.7594 - val_accuracy: 0.8532 - lr: 1.7052e-05\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 1.1841835758888112e-05.\n","Epoch 23/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4497 - accuracy: 0.8359\n","Epoch 23: val_accuracy did not improve from 0.86133\n","97/97 [==============================] - 53s 550ms/step - loss: 0.4497 - accuracy: 0.8359 - val_loss: 0.7476 - val_accuracy: 0.8571 - lr: 1.1842e-05\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 8.110846340981606e-06.\n","Epoch 24/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4264 - accuracy: 0.8458\n","Epoch 24: val_accuracy did not improve from 0.86133\n","97/97 [==============================] - 53s 551ms/step - loss: 0.4264 - accuracy: 0.8458 - val_loss: 0.7542 - val_accuracy: 0.8564 - lr: 8.1108e-06\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 5.480301665925623e-06.\n","Epoch 25/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4422 - accuracy: 0.8322\n","Epoch 25: val_accuracy did not improve from 0.86133\n","97/97 [==============================] - 53s 550ms/step - loss: 0.4422 - accuracy: 0.8322 - val_loss: 0.7496 - val_accuracy: 0.8590 - lr: 5.4803e-06\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 3.6535345013059364e-06.\n","Epoch 26/50\n","97/97 [==============================] - ETA: 0s - loss: 0.4366 - accuracy: 0.8362\n","Epoch 26: val_accuracy did not improve from 0.86133\n","Restoring model weights from the end of the best epoch: 19.\n","97/97 [==============================] - 54s 554ms/step - loss: 0.4366 - accuracy: 0.8362 - val_loss: 0.7587 - val_accuracy: 0.8538 - lr: 3.6535e-06\n","Epoch 26: early stopping\n"]}],"source":["history = model.fit(\n","            train_generator,\n","            steps_per_epoch = train_generator.samples // hparams.batch_size,\n","            validation_data = validation_generator, \n","            validation_steps = validation_generator.samples // hparams.batch_size,\n","            epochs = hparams.no_epochs,\n","            callbacks=[callbacks])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15619,"status":"ok","timestamp":1656463108683,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"9FCJXLCqrk6O","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef33fcbf-bc7a-4a27-c884-35a2bcadf0bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["48/48 [==============================] - 15s 320ms/step - loss: 0.7239 - accuracy: 0.8613\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.7239118218421936, 0.861328125]"]},"metadata":{},"execution_count":49}],"source":["model.evaluate(validation_generator, steps=validation_generator.samples // hparams.batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16103,"status":"ok","timestamp":1656463124777,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"vYQ-LaTppGVy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"927e9f65-2a3e-4ba6-eaf8-b83d3fb7bae8"},"outputs":[{"output_type":"stream","name":"stdout","text":["48/48 [==============================] - 15s 322ms/step - loss: 0.7140 - accuracy: 0.8646\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.7139595150947571, 0.8645833134651184]"]},"metadata":{},"execution_count":50}],"source":["model.evaluate(test_generator, steps=test_generator.samples // hparams.batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2375,"status":"ok","timestamp":1656463127143,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"PrH3LGDhtZ1y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"763a7a59-8827-4a26-f2a7-522e78d3c6de"},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/Portofolio/fruit_quant_aware/saved_models/assets\n"]}],"source":["model.save(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/saved_models/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVqCkYYMtZ8-","executionInfo":{"status":"ok","timestamp":1656463127601,"user_tz":240,"elapsed":460,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"colab":{"base_uri":"https://localhost:8080/","height":645},"outputId":"8716183e-323e-46b0-8739-654da753d23e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 600x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["#Evaluate the tensor size for the intermediate tensors (H*W*D)\n","\n","fig = plt.figure(dpi=100)\n","\n","ax = fig.add_axes([0,0,1,1])\n","\n","l_idx   = []\n","l_sizes = []\n","\n","for layer in model.layers[1:]:\n","  shape = layer.output_shape\n","  shape = np.delete(shape, 0)\n","  size  = np.prod(shape)\n","  l_idx   = np.append(l_idx, layer.name)\n","  l_sizes = np.append(l_sizes, size)\n","\n","ax.bar(l_idx, l_sizes)\n","plt.xticks(rotation='vertical')\n","plt.show()"]},{"cell_type":"markdown","source":["**IMPORTANT**\n","\n","We can have a rough estimation of the tensor arena by adding the size of the biGgest tensor multiplied by 2 and the size of the input and output. For our case the biggest tensor has roughly 35k parameters which means that when quantized it will need 35k *8bit size = 35 KB(35*1024B), 70KB when multiplied. As for the input it will be 48*48*3 parameters of 8bit each. Thus the input will need 7KB(7*1024). We will also need 1B for the output. In reality tensor arena will need more space cause it needs to accomodate more parameters.\n","\n","By our estimation it will need roughly 80KB"],"metadata":{"id":"fZwdqDpe4S5Q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNFfesYSeevh"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"k9vL3DpOfUbw"},"source":["## Quantization aware training model and finetune it\n"]},{"cell_type":"markdown","source":["#### Note that the resulting model is quantization aware but not quantized (e.g. the weights are float32 instead of int8). \n","\n","\n","**THE MODEL ITSELF ISN'T QUANTIZED.THE MODEL LEARNS PARAMETERS THAT ARE MORE ROBUST TO QUANTIZATION.**\n","\n","https://www.tensorflow.org/model_optimization/guide/quantization/training_example\n","\n","https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html"],"metadata":{"id":"c7UOCEeNSMWH"}},{"cell_type":"markdown","source":["#### quantize spacific type of layers"],"metadata":{"id":"6cckRvt5gaz8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKaVgCDdYz2I"},"outputs":[],"source":["# def apply_quantization_to_dense(layer):\n","#   if isinstance(layer, tf.keras.layers.Dense):\n","#     return tfmot.quantization.keras.quantize_annotate_layer(layer)\n","#   return layer\n","\n","#quant dense"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVS2sXvAaExX"},"outputs":[],"source":["# def apply_quantization_to_dense(layer):\n","#   if isinstance(layer, tf.keras.layers.Conv2D):\n","#     return tfmot.quantization.keras.quantize_annotate_layer(layer)\n","#   return layer\n","\n","#quant conv"]},{"cell_type":"code","source":["def apply_quantization_to_dense(layer):\n","  if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Dense):\n","    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n","  return layer\n","\n","\n","#quant conv and dense"],"metadata":{"id":"jwGyCSbRhDVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sa3CC-TVY0XR"},"outputs":[],"source":["annotated_model = tf.keras.models.clone_model(\n","    model,\n","    clone_function=apply_quantization_to_dense,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XelRiXTwY0aw"},"outputs":[],"source":["annotated_model.build((None, hparams.dim3d[0],hparams.dim3d[1],hparams.dim3d[2]))\n","\n","# Now that the Dense layers are annotated,\n","# `quantize_apply` actually makes the model quantization aware.\n","quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)"]},{"cell_type":"code","source":[""],"metadata":{"id":"VffFe7I3gkEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### quantize everything"],"metadata":{"id":"diWxknm4gykg"}},{"cell_type":"code","source":["# import tensorflow_model_optimization as tfmot\n","\n","# quantize_model = tfmot.quantization.keras.quantize_model\n","\n","# # q_aware stands for for quantization aware.\n","# quant_aware_model = quantize_model(model)"],"metadata":{"id":"_FxCCXrtglCt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"4sC8mNiJglMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKL0XlZeZ8QM"},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["#### finetune and perform quant aware training"],"metadata":{"id":"oazQSnBXhW17"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lx37C238Y0eO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656463130430,"user_tz":240,"elapsed":25,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"78f22e69-f6a0-4881-be67-45625f1042de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," quantize_layer (QuantizeLay  (None, 48, 48, 3)        3         \n"," er)                                                             \n","                                                                 \n"," quant_conv2d_6 (QuantizeWra  (None, 46, 46, 16)       483       \n"," pperV2)                                                         \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 46, 46, 16)       64        \n"," hNormalization)                                                 \n","                                                                 \n"," quant_conv2d_7 (QuantizeWra  (None, 44, 44, 16)       2355      \n"," pperV2)                                                         \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 44, 44, 16)       64        \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 22, 22, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," quant_dropout_3 (QuantizeWr  (None, 22, 22, 16)       1         \n"," apperV2)                                                        \n","                                                                 \n"," quant_conv2d_8 (QuantizeWra  (None, 20, 20, 32)       4707      \n"," pperV2)                                                         \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 20, 20, 32)       128       \n"," hNormalization)                                                 \n","                                                                 \n"," quant_conv2d_9 (QuantizeWra  (None, 18, 18, 32)       9315      \n"," pperV2)                                                         \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 18, 18, 32)       128       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 9, 9, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," quant_dropout_4 (QuantizeWr  (None, 9, 9, 32)         1         \n"," apperV2)                                                        \n","                                                                 \n"," quant_conv2d_10 (QuantizeWr  (None, 7, 7, 64)         18627     \n"," apperV2)                                                        \n","                                                                 \n"," batch_normalization_10 (Bat  (None, 7, 7, 64)         256       \n"," chNormalization)                                                \n","                                                                 \n"," quant_conv2d_11 (QuantizeWr  (None, 5, 5, 64)         37059     \n"," apperV2)                                                        \n","                                                                 \n"," batch_normalization_11 (Bat  (None, 5, 5, 64)         256       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 2, 2, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_5 (Dropout)         (None, 2, 2, 64)          0         \n","                                                                 \n"," quant_global_average_poolin  (None, 64)               3         \n"," g2d_1 (QuantizeWrapperV2)                                       \n","                                                                 \n"," quant_dense_1 (QuantizeWrap  (None, 24)               1565      \n"," perV2)                                                          \n","                                                                 \n","=================================================================\n","Total params: 75,015\n","Trainable params: 74,088\n","Non-trainable params: 927\n","_________________________________________________________________\n"]}],"source":["quant_aware_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-u4crJxwY0gY"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHIkwBD_bc7I"},"outputs":[],"source":["'''\n","Adding Callbacks and EarlyStopping\n","Callbacks and Checkpoints help to keep an eye on model while training and stop the training\n","if the performance has reached an optimum.\n","'''\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","filepath = 'Callbacks/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5'\n","checkpoint = ModelCheckpoint(filepath, monitor = 'val_accuracy', \n","                             verbose = 1,\n","                             save_best_only = True,\n","                             mode = 'max',\n","                             save_freq = \"epoch\", #check and save at the end of the epoch   \n","                             save_weights_only=False,   #save model too   \n","                             )#best accuracy saved\n","\n","early_stop = EarlyStopping(monitor = 'val_loss',\n","                           patience = 7, #wait 7 epochs before you restore best weights and stop model trainng\n","                           mode=\"min\", \n","                           verbose = 1,\n","                           min_delta=0.01,\n","                           restore_best_weights=True)#go to the model that had the best accuracy before the early stopping before patience epochs\n","\n","#https://keras.io/api/callbacks/model_checkpoint/\n","#https://keras.io/api/callbacks/early_stopping/\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3oH6ejbbkFn"},"outputs":[],"source":["#https://towardsdatascience.com/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c\n","#time decay\n","from tensorflow.keras.optimizers import Adam\n","\n","\n","\n","def lr_time_based_decay(epoch, lr):\n","    initial_learning_rate = hparams.lr /100\n","    epochs = hparams.no_epochs\n","    decay = initial_learning_rate / (epochs) *1000\n","\n","    return lr * 1 / (1 + decay * epoch)\n","\n","time_decay_learning_rate = tf.keras.callbacks.LearningRateScheduler (lr_time_based_decay, verbose=1) #CALLBACK \n","callbacks = [checkpoint, early_stop,time_decay_learning_rate]\n","optimizer = Adam(learning_rate=hparams.lr/100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5a28qs4ZSo-"},"outputs":[],"source":["# `quantize_model` requires a recompile.\n","quant_aware_model.compile(optimizer=optimizer,\n","              loss=\"categorical_crossentropy\",\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270088,"status":"ok","timestamp":1656463400512,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"oao4h8fVsHG2","outputId":"8183574a-50e0-40ea-e94c-623fcc0d4af7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-06.\n","Epoch 1/5\n","97/97 [==============================] - ETA: 0s - loss: 0.7798 - accuracy: 0.7226\n","Epoch 1: val_accuracy improved from -inf to 0.74707, saving model to Callbacks/weights-improvement-01-0.75.hdf5\n","97/97 [==============================] - 55s 557ms/step - loss: 0.7798 - accuracy: 0.7226 - val_loss: 1.0060 - val_accuracy: 0.7471 - lr: 1.0000e-05\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.998000147349282e-06.\n","Epoch 2/5\n","97/97 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.7738\n","Epoch 2: val_accuracy improved from 0.74707 to 0.83594, saving model to Callbacks/weights-improvement-02-0.84.hdf5\n","97/97 [==============================] - 53s 552ms/step - loss: 0.6498 - accuracy: 0.7738 - val_loss: 0.8223 - val_accuracy: 0.8359 - lr: 9.9980e-06\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.994002167662488e-06.\n","Epoch 3/5\n","97/97 [==============================] - ETA: 0s - loss: 0.5988 - accuracy: 0.7873\n","Epoch 3: val_accuracy improved from 0.83594 to 0.83757, saving model to Callbacks/weights-improvement-03-0.84.hdf5\n","97/97 [==============================] - 54s 557ms/step - loss: 0.5988 - accuracy: 0.7873 - val_loss: 0.7892 - val_accuracy: 0.8376 - lr: 9.9940e-06\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.988009733475177e-06.\n","Epoch 4/5\n","97/97 [==============================] - ETA: 0s - loss: 0.5481 - accuracy: 0.7984\n","Epoch 4: val_accuracy improved from 0.83757 to 0.84603, saving model to Callbacks/weights-improvement-04-0.85.hdf5\n","97/97 [==============================] - 53s 550ms/step - loss: 0.5481 - accuracy: 0.7984 - val_loss: 0.7528 - val_accuracy: 0.8460 - lr: 9.9880e-06\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.980025858038849e-06.\n","Epoch 5/5\n","97/97 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.7981\n","Epoch 5: val_accuracy improved from 0.84603 to 0.85905, saving model to Callbacks/weights-improvement-05-0.86.hdf5\n","97/97 [==============================] - 54s 554ms/step - loss: 0.5358 - accuracy: 0.7981 - val_loss: 0.7392 - val_accuracy: 0.8590 - lr: 9.9800e-06\n"]}],"source":["history = quant_aware_model.fit(\n","            train_generator,\n","            steps_per_epoch = train_generator.samples // hparams.batch_size,\n","            validation_data = validation_generator, \n","            validation_steps = validation_generator.samples // hparams.batch_size,\n","            epochs = 5,\n","            callbacks=[callbacks]\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15538,"status":"ok","timestamp":1656463416036,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"B0QwmqqdsHJk","outputId":"1cb49c2a-c745-4fc0-dade-7c901ea7c5c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["48/48 [==============================] - 15s 320ms/step - loss: 0.7392 - accuracy: 0.8590\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.7392217516899109, 0.8590494990348816]"]},"metadata":{},"execution_count":61}],"source":["quant_aware_model.evaluate(validation_generator, steps=validation_generator.samples // hparams.batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15792,"status":"ok","timestamp":1656463431818,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"aUebYthAsHLi","outputId":"9b513a0d-63ea-4f0f-a6b9-7d25dcebfb01"},"outputs":[{"output_type":"stream","name":"stdout","text":["48/48 [==============================] - 15s 322ms/step - loss: 0.7282 - accuracy: 0.8613\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.7282493710517883, 0.861328125]"]},"metadata":{},"execution_count":62}],"source":["quant_aware_model.evaluate(test_generator, steps=test_generator.samples // hparams.batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4600,"status":"ok","timestamp":1656464115946,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"dsB9a_eLsHQN","outputId":"9df54ace-04fd-4b00-d779-645c1f2cea17"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, dropout_3_layer_call_fn while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/Portofolio/fruit_quant_aware/quant_saved_models/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/Portofolio/fruit_quant_aware/quant_saved_models/assets\n"]}],"source":["quant_aware_model.save(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/quant_saved_models/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dFWRTAJ8sHdY"},"outputs":[],"source":["temp = tf.keras.models.load_model('quant_saved_models')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15533,"status":"ok","timestamp":1656464151987,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"2AIoZP5m2xek","outputId":"2b978d2e-b583-4b8a-bd21-3ef6f1f79fc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["48/48 [==============================] - 15s 314ms/step - loss: 0.7392 - accuracy: 0.8590\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.7392217516899109, 0.8590494990348816]"]},"metadata":{},"execution_count":68}],"source":["temp.evaluate(validation_generator, steps=validation_generator.samples // hparams.batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16036,"status":"ok","timestamp":1656464168013,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"iYn1_Dko2xiZ","outputId":"f8be42f8-0a52-46ed-ea1a-9b2bec1841a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["48/48 [==============================] - 15s 318ms/step - loss: 0.7283 - accuracy: 0.8607\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.7283408045768738, 0.8606770634651184]"]},"metadata":{},"execution_count":69}],"source":["temp.evaluate(test_generator, steps=test_generator.samples // hparams.batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tckmW3dQ2xmL"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChfLMSLZsHek"},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["## SRAM MEMORY OF ARDUINO IS 256KB/0.25MB MODEL MUST FIT THERE"],"metadata":{"id":"cG7wRH4lJF--"}},{"cell_type":"markdown","source":["## Convert to TFLite"],"metadata":{"id":"9JeWvDr4svV5"}},{"cell_type":"code","source":["# labels = glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*\")\n","# labels = list(map(lambda x: x.split(\"/\")[-1], labels))\n","# labels[:2]"],"metadata":{"id":"dSSfuXP2-6lT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = list(train_generator.class_indices )\n","labels[:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAE_HKgsB0QV","executionInfo":{"status":"ok","timestamp":1656465464792,"user_tz":240,"elapsed":5,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"b49166a7-c573-4978-adda-963da2c54298"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['apple_6', 'apple_braeburn_1']"]},"metadata":{},"execution_count":114}]},{"cell_type":"markdown","metadata":{"id":"Jl03f9MyzFQA"},"source":["## Quantization Aware Model Full-Integer Quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdoBtULg7PaI"},"outputs":[],"source":["def representative_data_gen():\n","\n","    imgs = tf.data.Dataset.from_tensor_slices(next(test_generator)[0]).batch(1)\n","    for i in imgs.take(64):#batch size\n","        i = tf.dtypes.cast(i, tf.float32)\n","        yield [i]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ax5XXgqO7PdY","outputId":"329156a6-58e8-4f1b-9895-fc6ab71acd63","executionInfo":{"status":"ok","timestamp":1656465927765,"user_tz":240,"elapsed":9552,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, dropout_3_layer_call_fn while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /tmp/tmpwr0mq49h/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /tmp/tmpwr0mq49h/assets\n","/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n","  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n","WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"]},{"output_type":"execute_result","data":{"text/plain":["91304"]},"metadata":{},"execution_count":118}],"source":["converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n","converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","# converter.target_spec.supported_ops=[tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","# converter.inference_input_type = tf.int8 #quant only the input\n","\n","tflite_model = converter.convert()\n","open(\"TFLite_Models/model.tflite\",\"wb\").write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXvxqt8m7Phw","executionInfo":{"status":"ok","timestamp":1656465932457,"user_tz":240,"elapsed":4705,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"29bacbcd-5a0c-4f10-f05e-fc73ff90f23c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [Connecting to security.ubuntu.com (91.189.91.38)] [Connected to cloud.r-pro\r                                                                               \rHit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Co\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","\r0% [1 InRelease gpgv 242 kB] [4 InRelease 12.7 kB/88.7 kB 14%] [Connecting to s\r                                                                               \rHit:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\r0% [1 InRelease gpgv 242 kB] [4 InRelease 15.6 kB/88.7 kB 18%] [Connecting to s\r                                                                               \rHit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 242 kB] [4 InRelease 37.4 kB/88.7 kB 42%] [Connecting to s\r0% [1 InRelease gpgv 242 kB] [Connecting to security.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rGet:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,298 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,065 kB]\n","Fetched 4,615 kB in 2s (2,682 kB/s)\n","Reading package lists... Done\n"]}],"source":["!apt-get update && apt-get -qq install xxd\n","!xxd -i TFLite_Models/model.tflite > TFLite_Models/model.h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHCql-bn7PkV","executionInfo":{"status":"ok","timestamp":1656465932457,"user_tz":240,"elapsed":3,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"04253800-f216-4ad0-ec62-83f82dc627fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Header file in MB: 0.5370559692382812\n","TFLite Model in MB: 0.08707427978515625\n","TFLite Model in KB: 89.1640625\n"]}],"source":["print(\"Header file in MB:\", os.path.getsize(\"TFLite_Models/model.h\") / float(2**20))\n","print(\"TFLite Model in MB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**20))\n","print(\"TFLite Model in KB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**10))\n","#print(len(tflite_model)/ float(2**20))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmKx13PX7Ppb"},"outputs":[],"source":["#accuracy evaluator\n","\n","# Initialize the TFLite interpreter\n","tfl_inter = tf.lite.Interpreter(model_content=tflite_model)\n","\n","# Allocate the tensors\n","tfl_inter.allocate_tensors()\n","\n","# Get input/output layer information\n","i_details = tfl_inter.get_input_details()[0]\n","o_details = tfl_inter.get_output_details()[0]\n","\n","def classify(i_data):\n","  \n","  input_data = i_data[np.newaxis, ...] #add batch dimension\n","  i_value_f32 = tf.dtypes.cast(input_data, tf.float32)\n","  \n","\n","  #leave input as it is\n","  i_value_s8 = i_value_f32\n","\n","  tfl_inter.set_tensor(i_details[\"index\"], i_value_s8)\n","  tfl_inter.invoke()\n","  o_pred = tfl_inter.get_tensor(o_details[\"index\"])[0]\n","\n","  # return (o_pred - o_zero_point) * o_scale\n","  return o_pred\n"]},{"cell_type":"code","source":["import PIL\n","from PIL import Image\n","print('Pillow Version:', PIL.__version__)\n","print(\"number of test images in total\", len(glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\")))"],"metadata":{"id":"eoFU9PeQFon_","executionInfo":{"status":"ok","timestamp":1656465932678,"user_tz":240,"elapsed":223,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"da41ca40-5134-4a6b-904b-b2bb2966f466"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pillow Version: 7.1.2\n","number of test images in total 6231\n"]}]},{"cell_type":"code","source":["num_correct_samples = 0\n","num_total_samples   = len(glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"))\n","\n","ind = 0\n","\n","for img_path in glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"):\n","  image = Image.open(img_path)\n","  image = image.resize(hparams.dim2d) #image resize\n","  image = np.array(image) #convert to numpy\n","  image = image/255.0 #standardize\n","\n","  pred = classify(image)\n","  label = (img_path.split(\"/\")[-2])#contains the true label\n","\n","  # print(labels[np.argmax(pred)],label)\n","  # break\n","  if labels[np.argmax(pred)]==label:\n","    num_correct_samples = num_correct_samples + 1\n","\n","  if ind%1000==0:\n","    print(f\"{ind+1} sample\")\n","  ind = ind + 1\n","  \n","\n","acc = num_correct_samples/num_total_samples\n","acc"],"metadata":{"id":"1qggxqkXHiB1","executionInfo":{"status":"ok","timestamp":1656466187007,"user_tz":240,"elapsed":254330,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4abbc301-c11d-4f64-801d-714252691122"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 sample\n","1001 sample\n","2001 sample\n","3001 sample\n","4001 sample\n","5001 sample\n","6001 sample\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8244262558176858"]},"metadata":{},"execution_count":123}]},{"cell_type":"markdown","metadata":{"id":"7H1wNq2pHGPD"},"source":["## Quantization Aware Model without quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8815,"status":"ok","timestamp":1656466195811,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"zPJu704v7PxG","outputId":"da7b1af8-b831-4669-d1ba-4d52977d70b3"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, dropout_3_layer_call_fn while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /tmp/tmp5cebjz5l/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /tmp/tmp5cebjz5l/assets\n","WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"]},{"output_type":"execute_result","data":{"text/plain":["617632"]},"metadata":{},"execution_count":124}],"source":["converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n","\n","tflite_model = converter.convert()\n","open(\"TFLite_Models/model.tflite\",\"wb\").write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4356,"status":"ok","timestamp":1656466200149,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"lhy22NFuHQ_V","outputId":"c943ce16-fc54-449c-9192-d9984cd00656"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [Connecting to security.ubuntu.com (91.189.91.38)] [Connecting to cloud.r-pr\r                                                                               \rHit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Co\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","\r0% [1 InRelease gpgv 242 kB] [Connecting to security.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","\r0% [1 InRelease gpgv 242 kB] [5 InRelease 11.3 kB/74.6 kB 15%] [Connecting to s\r                                                                               \rHit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 242 kB] [5 InRelease 17.1 kB/74.6 kB 23%] [Connecting to s\r                                                                               \rHit:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\r0% [1 InRelease gpgv 242 kB] [5 InRelease 17.1 kB/74.6 kB 23%] [Connecting to s\r                                                                               \rHit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Fetched 163 kB in 2s (97.7 kB/s)\n","Reading package lists... Done\n"]}],"source":["!apt-get update && apt-get -qq install xxd\n","!xxd -i TFLite_Models/model.tflite > TFLite_Models/model.h"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1656466200149,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"FfwYnan27P00","outputId":"be1f76e8-2107-42c5-bf3c-7797b5019db1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Header file in MB: 3.632388114929199\n","TFLite Model in MB: 0.589019775390625\n","TFLite Model in KB: 603.15625\n"]}],"source":["print(\"Header file in MB:\", os.path.getsize(\"TFLite_Models/model.h\") / float(2**20))\n","print(\"TFLite Model in MB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**20))\n","print(\"TFLite Model in KB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**10))\n","#print(len(tflite_model)/ float(2**20))"]},{"cell_type":"code","source":["#accuracy evaluator\n","\n","# Initialize the TFLite interpreter\n","tfl_inter = tf.lite.Interpreter(model_content=tflite_model)\n","\n","# Allocate the tensors\n","tfl_inter.allocate_tensors()\n","\n","# Get input/output layer information\n","i_details = tfl_inter.get_input_details()[0]\n","o_details = tfl_inter.get_output_details()[0]\n","\n","\n","def classify(i_data):\n","  \n","  input_data = i_data[np.newaxis, ...] #add batch dimension\n","  i_value_f32 = tf.dtypes.cast(input_data, tf.float32)\n","  \n","\n","  #leave input as it is\n","  i_value_s8 = i_value_f32\n","\n","  tfl_inter.set_tensor(i_details[\"index\"], i_value_s8)\n","  tfl_inter.invoke()\n","  o_pred = tfl_inter.get_tensor(o_details[\"index\"])[0]\n","\n","  # return (o_pred - o_zero_point) * o_scale\n","  return o_pred\n"],"metadata":{"id":"E36L-ItkODTt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PIL\n","from PIL import Image\n","print('Pillow Version:', PIL.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5D-YUxXODYr","executionInfo":{"status":"ok","timestamp":1656466200371,"user_tz":240,"elapsed":228,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"992efc2e-268c-453c-d504-ee10f8a8294c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pillow Version: 7.1.2\n"]}]},{"cell_type":"code","source":["num_correct_samples = 0\n","num_total_samples   = len(glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"))\n","\n","ind = 0\n","\n","for img_path in glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"):\n","  image = Image.open(img_path)\n","  image = image.resize(hparams.dim2d) #image resize\n","  image = np.array(image) #convert to numpy\n","  image = image/255.0 #standardize\n","\n","  pred = classify(image)\n","  label = (img_path.split(\"/\")[-2])#contains the true label\n","\n","  # print(labels[np.argmax(pred)],label)\n","  # break\n","  if labels[np.argmax(pred)]==label:\n","    num_correct_samples = num_correct_samples + 1\n","\n","  if ind%1000==0:\n","    print(f\"{ind+1} sample\")\n","  ind = ind + 1\n","  \n","\n","acc = num_correct_samples/num_total_samples\n","acc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OV8EFGe0ODa_","executionInfo":{"status":"ok","timestamp":1656466259389,"user_tz":240,"elapsed":59021,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"4b24d72e-d360-4c76-893a-e163832b0036"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 sample\n","1001 sample\n","2001 sample\n","3001 sample\n","4001 sample\n","5001 sample\n","6001 sample\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8425613866153105"]},"metadata":{},"execution_count":129}]},{"cell_type":"markdown","metadata":{"id":"KiQlOB7ut5pH"},"source":["\n","## Original model Full Integer Quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAknakG2wUxz"},"outputs":[],"source":["def representative_data_gen():\n","\n","    imgs = tf.data.Dataset.from_tensor_slices(next(test_generator)[0]).batch(1)\n","    for i in imgs.take(64):#batch size\n","        i = tf.dtypes.cast(i, tf.float32)\n","        yield [i]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5054,"status":"ok","timestamp":1656466264441,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"rKtEkkPAtun2","outputId":"a22b8ef7-a402-46d4-c892-ea4692ccaf4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /tmp/tmpt5w54fc2/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /tmp/tmpt5w54fc2/assets\n","/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n","  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n","WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"]},{"output_type":"execute_result","data":{"text/plain":["90440"]},"metadata":{},"execution_count":131}],"source":["converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","\n","tflite_model = converter.convert()\n","open(\"TFLite_Models/model.tflite\",\"wb\").write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4620,"status":"ok","timestamp":1656466269052,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"29fnpmAUtuq1","outputId":"650f0150-7b52-4023-82fa-1c268dbe3e0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [Connecting to security.ubuntu.com (91.189.91.38)] [Connected to cloud.r-pro\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rHit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","\r0% [1 InRelease gpgv 242 kB] [Connecting to security.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rHit:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","\r0% [1 InRelease gpgv 242 kB] [6 InRelease 11.3 kB/74.6 kB 15%] [Connecting to s\r                                                                               \rHit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 242 kB] [6 InRelease 14.2 kB/74.6 kB 19%] [Connecting to s\r                                                                               \rHit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Fetched 163 kB in 2s (96.6 kB/s)\n","Reading package lists... Done\n"]}],"source":["!apt-get update && apt-get -qq install xxd\n","!xxd -c 60 -i TFLite_Models/model.tflite > TFLite_Models/model.h"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1656466269052,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"},"user_tz":240},"id":"5Mr8tEq8uJhR","outputId":"cfd5736f-8f82-4a9e-8509-860a58122b32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Header file in MB: 0.5204753875732422\n","TFLite Model in MB: 0.08625030517578125\n","TFLite Model in KB: 88.3203125\n"]}],"source":["print(\"Header file in MB:\", os.path.getsize(\"TFLite_Models/model.h\") / float(2**20))\n","print(\"TFLite Model in MB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**20))\n","print(\"TFLite Model in KB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**10))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBw7sGT9uJlj"},"outputs":[],"source":["#accuracy evaluator\n","\n","# Initialize the TFLite interpreter\n","tfl_inter = tf.lite.Interpreter(model_content=tflite_model)\n","\n","# Allocate the tensors\n","tfl_inter.allocate_tensors()\n","\n","# Get input/output layer information\n","i_details = tfl_inter.get_input_details()[0]\n","o_details = tfl_inter.get_output_details()[0]\n","\n","\n","\n","def classify(i_data):\n","  \n","  input_data = i_data[np.newaxis, ...] #add batch dimension\n","  i_value_f32 = tf.dtypes.cast(input_data, tf.float32)\n","  \n","\n","  #leave input as it is\n","  i_value_s8 = i_value_f32\n","\n","  tfl_inter.set_tensor(i_details[\"index\"], i_value_s8)\n","  tfl_inter.invoke()\n","  o_pred = tfl_inter.get_tensor(o_details[\"index\"])[0]\n","\n","  # return (o_pred - o_zero_point) * o_scale\n","  return o_pred\n"]},{"cell_type":"code","source":["import PIL\n","from PIL import Image\n","print('Pillow Version:', PIL.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9VKDQ5vONh-","executionInfo":{"status":"ok","timestamp":1656466269291,"user_tz":240,"elapsed":5,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"62de5a73-8e28-40aa-ff05-53f9a075093a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pillow Version: 7.1.2\n"]}]},{"cell_type":"code","source":["num_correct_samples = 0\n","num_total_samples   = len(glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"))\n","\n","ind = 0\n","\n","for img_path in glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"):\n","  image = Image.open(img_path)\n","  image = image.resize(hparams.dim2d) #image resize\n","  image = np.array(image) #convert to numpy\n","  image = image/255.0 #standardize\n","\n","  pred = classify(image)\n","  label = (img_path.split(\"/\")[-2])#contains the true label\n","\n","  # print(labels[np.argmax(pred)],label)\n","  # break\n","  if labels[np.argmax(pred)]==label:\n","    num_correct_samples = num_correct_samples + 1\n","\n","  if ind%1000==0:\n","    print(f\"{ind+1} sample\")\n","  ind = ind + 1\n","  \n","\n","acc = num_correct_samples/num_total_samples\n","acc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JC5GYB1RONnS","executionInfo":{"status":"ok","timestamp":1656466523119,"user_tz":240,"elapsed":253831,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"cb3e6794-90d0-449a-8e46-a34c7601713b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 sample\n","1001 sample\n","2001 sample\n","3001 sample\n","4001 sample\n","5001 sample\n","6001 sample\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8332530893917509"]},"metadata":{},"execution_count":136}]},{"cell_type":"markdown","source":["## Original model without quantization"],"metadata":{"id":"R9YYvxuWDMsg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmeipMUxuJrH","executionInfo":{"status":"ok","timestamp":1656466528035,"user_tz":240,"elapsed":4927,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"54b4be38-e511-4dbf-8dc5-ec1017736de2"},"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /tmp/tmpy2z6qi_y/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /tmp/tmpy2z6qi_y/assets\n","WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"]},{"output_type":"execute_result","data":{"text/plain":["304692"]},"metadata":{},"execution_count":137}],"source":["converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","\n","tflite_model = converter.convert()\n","open(\"TFLite_Models/model.tflite\",\"wb\").write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2RWpYa6tutF","executionInfo":{"status":"ok","timestamp":1656466532700,"user_tz":240,"elapsed":4675,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a0677044-abd0-470a-a320-60a420d614d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [Connecting to security.ubuntu.com (91.189.91.38)] [Connected to cloud.r-pro\r                                                                               \rHit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Wa\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","\r0% [1 InRelease gpgv 242 kB] [Connecting to security.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rHit:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rGet:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","\r0% [1 InRelease gpgv 242 kB] [7 InRelease 11.3 kB/74.6 kB 15%] [Connecting to s\r                                                                               \rHit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Fetched 163 kB in 2s (95.5 kB/s)\n","Reading package lists... Done\n"]}],"source":["!apt-get update && apt-get -qq install xxd\n","!xxd -c 60 -i TFLite_Models/model.tflite > TFLite_Models/model.h"]},{"cell_type":"code","source":["print(\"Header file in MB:\", os.path.getsize(\"TFLite_Models/model.h\") / float(2**20))\n","print(\"TFLite Model in MB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**20))\n","print(\"TFLite Model in KB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**10))"],"metadata":{"id":"A6V2q5wUD3cW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656466532701,"user_tz":240,"elapsed":22,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"f7add779-3948-4373-d08e-e2071395266f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Header file in MB: 1.7532472610473633\n","TFLite Model in MB: 0.2905769348144531\n","TFLite Model in KB: 297.55078125\n"]}]},{"cell_type":"code","source":["#accuracy evaluator\n","\n","# Initialize the TFLite interpreter\n","tfl_inter = tf.lite.Interpreter(model_content=tflite_model)\n","\n","# Allocate the tensors\n","tfl_inter.allocate_tensors()\n","\n","# Get input/output layer information\n","i_details = tfl_inter.get_input_details()[0]\n","o_details = tfl_inter.get_output_details()[0]\n","\n","\n","def classify(i_data):\n","  \n","  input_data = i_data[np.newaxis, ...] #add batch dimension\n","  i_value_f32 = tf.dtypes.cast(input_data, tf.float32)\n","  \n","  #leave input as it is\n","  i_value_s8 = i_value_f32\n","\n","  tfl_inter.set_tensor(i_details[\"index\"], i_value_s8)\n","  tfl_inter.invoke()\n","  o_pred = tfl_inter.get_tensor(o_details[\"index\"])[0]\n","\n","  # return (o_pred - o_zero_point) * o_scale\n","  return o_pred"],"metadata":{"id":"62kJwdGuOVB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PIL\n","from PIL import Image\n","print('Pillow Version:', PIL.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bt3nsxk5OVFx","executionInfo":{"status":"ok","timestamp":1656466532931,"user_tz":240,"elapsed":16,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"afebce05-88b9-4c9b-e824-460521864a19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pillow Version: 7.1.2\n"]}]},{"cell_type":"code","source":["num_correct_samples = 0\n","num_total_samples   = len(glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"))\n","\n","ind = 0\n","\n","for img_path in glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"):\n","  image = Image.open(img_path)\n","  image = image.resize(hparams.dim2d) #image resize\n","  image = np.array(image) #convert to numpy\n","  image = image/255.0 #standardize\n","\n","  pred = classify(image)\n","  label = (img_path.split(\"/\")[-2])#contains the true label\n","\n","  # print(labels[np.argmax(pred)],label)\n","  # break\n","  if labels[np.argmax(pred)]==label:\n","    num_correct_samples = num_correct_samples + 1\n","\n","  if ind%1000==0:\n","    print(f\"{ind+1} sample\")\n","  ind = ind + 1\n","  \n","\n","acc = num_correct_samples/num_total_samples\n","acc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKYz1L4qOVH3","executionInfo":{"status":"ok","timestamp":1656466574153,"user_tz":240,"elapsed":41235,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"00eb92ff-69e7-4be4-b42a-187a2304b238"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 sample\n","1001 sample\n","2001 sample\n","3001 sample\n","4001 sample\n","5001 sample\n","6001 sample\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8351789439897288"]},"metadata":{},"execution_count":142}]},{"cell_type":"markdown","source":["## Quantization Aware Model Full Integer Quantization with Input Quantization too"],"metadata":{"id":"0MfCJoZrMnzB"}},{"cell_type":"code","source":["def representative_data_gen():\n","\n","    imgs = tf.data.Dataset.from_tensor_slices(next(test_generator)[0]).batch(1)\n","    for i in imgs.take(64):#batch size\n","        i = tf.dtypes.cast(i, tf.float32)\n","        yield [i]"],"metadata":{"id":"vpp-6tnSgyDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n","converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter.inference_input_type = tf.int8\n","\n","tflite_model = converter.convert()\n","open(\"TFLite_Models/model.tflite\",\"wb\").write(tflite_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxIjVCwlNBT_","executionInfo":{"status":"ok","timestamp":1656466583360,"user_tz":240,"elapsed":9217,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"8d31675e-9ef6-454d-a03b-e20d339a4ca4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn, conv2d_7_layer_call_and_return_conditional_losses, dropout_3_layer_call_fn while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /tmp/tmpga0xtvw7/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /tmp/tmpga0xtvw7/assets\n","/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n","  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n","WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"]},{"output_type":"execute_result","data":{"text/plain":["91008"]},"metadata":{},"execution_count":144}]},{"cell_type":"code","source":["!apt-get update && apt-get -qq install xxd\n","!xxd -c 60 -i TFLite_Models/model.tflite > TFLite_Models/model.h"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqd65R-PNBXh","executionInfo":{"status":"ok","timestamp":1656466588076,"user_tz":240,"elapsed":4728,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"5d9ac5a2-22a1-4416-c59d-a1085301dd80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Connecting to security.\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 15.9 kB] [Connecting to security.ubuntu.com (91.189.91.38)\r                                                                               \rHit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","\r0% [1 InRelease gpgv 15.9 kB] [Connecting to security.ubuntu.com (91.189.91.38)\r                                                                               \rGet:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","\r0% [1 InRelease gpgv 15.9 kB] [7 InRelease 11.3 kB/74.6 kB 15%] [Connecting to \r                                                                               \rHit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Fetched 163 kB in 2s (98.1 kB/s)\n","Reading package lists... Done\n"]}]},{"cell_type":"code","source":["print(\"Header file in MB:\", os.path.getsize(\"TFLite_Models/model.h\") / float(2**20))\n","print(\"TFLite Model in MB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**20))\n","print(\"TFLite Model in KB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**10))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpduPtQpNBaq","executionInfo":{"status":"ok","timestamp":1656466588079,"user_tz":240,"elapsed":23,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"cd03211d-6af9-41b3-bd86-9265631aa5b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Header file in MB: 0.52374267578125\n","TFLite Model in MB: 0.0867919921875\n","TFLite Model in KB: 88.875\n"]}]},{"cell_type":"code","source":["#accuracy evaluator\n","\n","# Initialize the TFLite interpreter\n","tfl_inter = tf.lite.Interpreter(model_content=tflite_model)\n","\n","# Allocate the tensors\n","tfl_inter.allocate_tensors()\n","\n","# Get input/output layer information\n","i_details = tfl_inter.get_input_details()[0]\n","o_details = tfl_inter.get_output_details()[0]\n","\n","i_quant = i_details[\"quantization_parameters\"]\n","o_quant = o_details[\"quantization_parameters\"]\n","i_scale      = i_quant['scales'][0]\n","i_zero_point = i_quant['zero_points'][0]\n","\n","\n","\n","def classify(i_data):\n","  \n","  input_data = i_data[np.newaxis, ...] #add batch dimension\n","  i_value_f32 = tf.dtypes.cast(input_data, tf.float32)\n","  \n","  # Quantize (float -> 8-bit) the input (check if input layer is 8-bit, first)\n","  i_value_f32 = i_value_f32 / i_scale + i_zero_point\n","  i_value_s8 = tf.cast(i_value_f32, dtype=tf.int8)\n","\n","\n","  tfl_inter.set_tensor(i_details[\"index\"], i_value_s8)\n","  tfl_inter.invoke()\n","  o_pred = tfl_inter.get_tensor(o_details[\"index\"])[0]\n","\n","  return o_pred\n"],"metadata":{"id":"owLwgaG9NBhJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PIL\n","from PIL import Image\n","print('Pillow Version:', PIL.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58dwi5hhNBiz","executionInfo":{"status":"ok","timestamp":1656466588081,"user_tz":240,"elapsed":22,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"856b8451-5710-47dc-d4dd-430f76a0e120"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pillow Version: 7.1.2\n"]}]},{"cell_type":"code","source":["num_correct_samples = 0\n","num_total_samples   = len(glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"))\n","\n","ind = 0\n","\n","for img_path in glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"):\n","  image = Image.open(img_path)\n","  image = image.resize(hparams.dim2d) #image resize\n","  image = np.array(image) #convert to numpy\n","  image = image/255.0 #standardize\n","\n","  pred = classify(image)\n","  label = (img_path.split(\"/\")[-2])#contains the true label\n","\n","  # print(labels[np.argmax(pred)],label)\n","  # break\n","  if labels[np.argmax(pred)]==label:\n","    num_correct_samples = num_correct_samples + 1\n","\n","  if ind%1000==0:\n","    print(f\"{ind+1} sample\")\n","  ind = ind + 1\n","  \n","\n","acc = num_correct_samples/num_total_samples\n","acc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAgA0iKaNBkK","executionInfo":{"status":"ok","timestamp":1656466844484,"user_tz":240,"elapsed":256421,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"3a7f1053-6ae3-4994-fd8b-9d809b945c5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 sample\n","1001 sample\n","2001 sample\n","3001 sample\n","4001 sample\n","5001 sample\n","6001 sample\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8194511314395763"]},"metadata":{},"execution_count":149}]},{"cell_type":"markdown","source":["## Original Model Full Integer Quantization with Input Quantization too"],"metadata":{"id":"-1XEE0UlZl2C"}},{"cell_type":"code","source":["def representative_data_gen():\n","\n","    imgs = tf.data.Dataset.from_tensor_slices(next(test_generator)[0]).batch(1)\n","    for i in imgs.take(64):#batch size\n","        i = tf.dtypes.cast(i, tf.float32)\n","        yield [i]"],"metadata":{"id":"65iz-nC4NBle"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter.inference_input_type = tf.int8\n","# converter.inference_output_type = tf.int8\n","\n","tflite_model = converter.convert()\n","open(\"TFLite_Models/model.tflite\",\"wb\").write(tflite_model)"],"metadata":{"id":"ciK8_2WIgyEn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656466850020,"user_tz":240,"elapsed":5538,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"347f57f0-e791-4f54-bba3-58dd36aec41c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /tmp/tmp5cgw7coi/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /tmp/tmp5cgw7coi/assets\n","/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n","  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n","WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"]},{"output_type":"execute_result","data":{"text/plain":["90256"]},"metadata":{},"execution_count":151}]},{"cell_type":"code","source":["!apt-get update && apt-get -qq install xxd\n","!xxd -c 60 -i TFLite_Models/model.tflite > TFLite_Models/model.h"],"metadata":{"id":"Ll3tseppgyFr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656466854716,"user_tz":240,"elapsed":4699,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"dcc2cd56-7ad1-48e8-83e9-55a004977afa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Waiting for headers] [Con\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [2 InRelease 14.2 kB/88.7 \r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.38)]\r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Fetched 163 kB in 2s (94.6 kB/s)\n","Reading package lists... Done\n"]}]},{"cell_type":"code","source":["print(\"Header file in MB:\", os.path.getsize(\"TFLite_Models/model.h\") / float(2**20))\n","print(\"TFLite Model in MB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**20))\n","print(\"TFLite Model in KB:\", os.path.getsize(\"TFLite_Models/model.tflite\") / float(2**10))\n","#print(len(tflite_model)/ float(2**20))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dn6SJeXOauT9","executionInfo":{"status":"ok","timestamp":1656466854717,"user_tz":240,"elapsed":6,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"1fdd5023-c6c5-4596-b743-4e5a6084b17d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Header file in MB: 0.5194168090820312\n","TFLite Model in MB: 0.0860748291015625\n","TFLite Model in KB: 88.140625\n"]}]},{"cell_type":"code","source":["#accuracy evaluator\n","\n","# Initialize the TFLite interpreter\n","tfl_inter = tf.lite.Interpreter(model_content=tflite_model)\n","\n","# Allocate the tensors\n","tfl_inter.allocate_tensors()\n","\n","# Get input/output layer information\n","i_details = tfl_inter.get_input_details()[0]\n","o_details = tfl_inter.get_output_details()[0]\n","\n","i_quant = i_details[\"quantization_parameters\"]\n","o_quant = o_details[\"quantization_parameters\"]\n","i_scale      = i_quant['scales'][0]\n","i_zero_point = i_quant['zero_points'][0]\n","\n","\n","\n","def classify(i_data):\n","  \n","  input_data = i_data[np.newaxis, ...] #add batch dimension\n","  i_value_f32 = tf.dtypes.cast(input_data, tf.float32)\n","  \n","  # Quantize (float -> 8-bit) the input (check if input layer is 8-bit, first)\n","  i_value_f32 = i_value_f32 / i_scale + i_zero_point\n","  i_value_s8 = tf.cast(i_value_f32, dtype=tf.int8)\n","\n","\n","\n","  tfl_inter.set_tensor(i_details[\"index\"], i_value_s8)\n","  tfl_inter.invoke()\n","  o_pred = tfl_inter.get_tensor(o_details[\"index\"])[0]\n","\n","  return o_pred\n"],"metadata":{"id":"LEv10B94auXr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PIL\n","from PIL import Image\n","print('Pillow Version:', PIL.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVyxHn61auZi","executionInfo":{"status":"ok","timestamp":1656466855075,"user_tz":240,"elapsed":3,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"16b5728e-cf7b-4d47-df3f-2578e60af94a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pillow Version: 7.1.2\n","number of test images in total 200\n"]}]},{"cell_type":"code","source":["num_correct_samples = 0\n","num_total_samples   = len(glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"))\n","\n","ind = 0\n","\n","for img_path in glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Training/*/*\"):\n","  image = Image.open(img_path)\n","  image = image.resize(hparams.dim2d) #image resize\n","  image = np.array(image) #convert to numpy\n","  image = image/255.0 #standardize\n","\n","  pred = classify(image)\n","  label = (img_path.split(\"/\")[-2])#contains the true label\n","\n","  # print(labels[np.argmax(pred)],label)\n","  # break\n","  if labels[np.argmax(pred)]==label:\n","    num_correct_samples = num_correct_samples + 1\n","\n","  if ind%1000==0:\n","    print(f\"{ind+1} sample\")\n","  ind = ind + 1\n","  \n","\n","acc = num_correct_samples/num_total_samples\n","acc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SDB0UYL3audz","executionInfo":{"status":"ok","timestamp":1656467111372,"user_tz":240,"elapsed":256298,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"4f5ece0a-81a7-487c-b133-14057b73ed2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 sample\n","1001 sample\n","2001 sample\n","3001 sample\n","4001 sample\n","5001 sample\n","6001 sample\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8478574867597496"]},"metadata":{},"execution_count":156}]},{"cell_type":"markdown","source":["## get all labels for arduino"],"metadata":{"id":"xUQp2bc0roJJ"}},{"cell_type":"code","source":["list(train_generator.class_indices )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvPTux44r_uR","executionInfo":{"status":"ok","timestamp":1656694570086,"user_tz":240,"elapsed":13,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"5f468580-d611-4e83-a36e-5207509d945d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['apple_6',\n"," 'apple_braeburn_1',\n"," 'apple_crimson_snow_1',\n"," 'apple_golden_1',\n"," 'apple_golden_2',\n"," 'apple_golden_3',\n"," 'apple_granny_smith_1',\n"," 'apple_hit_1',\n"," 'apple_pink_lady_1',\n"," 'apple_red_1',\n"," 'apple_red_2',\n"," 'apple_red_3',\n"," 'apple_red_delicios_1',\n"," 'apple_red_yellow_1',\n"," 'apple_rotten_1',\n"," 'cabbage_white_1',\n"," 'carrot_1',\n"," 'cucumber_1',\n"," 'cucumber_3',\n"," 'eggplant_violet_1',\n"," 'pear_1',\n"," 'pear_3',\n"," 'zucchini_1',\n"," 'zucchini_dark_1']"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["## create an image as example"],"metadata":{"id":"OmxBPoLnjX2p"}},{"cell_type":"code","source":["!ls Test "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XeECoMkOsTHa","executionInfo":{"status":"ok","timestamp":1656694609776,"user_tz":240,"elapsed":297,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"a906b47f-3977-41f1-a8ca-74957d8a2949"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["apple_6\t\t      apple_pink_lady_1     carrot_1\n","apple_braeburn_1      apple_red_1\t    cucumber_1\n","apple_crimson_snow_1  apple_red_2\t    cucumber_3\n","apple_golden_1\t      apple_red_3\t    eggplant_violet_1\n","apple_golden_2\t      apple_red_delicios_1  pear_1\n","apple_golden_3\t      apple_red_yellow_1    pear_3\n","apple_granny_smith_1  apple_rotten_1\t    zucchini_1\n","apple_hit_1\t      cabbage_white_1\t    zucchini_dark_1\n"]}]},{"cell_type":"code","source":["import sys\n","import numpy\n","np.set_printoptions(threshold=sys.maxsize)\n","\n","for img_path in glob.glob(\"/content/drive/MyDrive/Portofolio/fruit_quant_aware/Test/*/*\"):\n","  image = Image.open(img_path)\n","  image = image.resize(hparams.dim2d) #image resize\n","  image = np.array(image) #convert to numpy\n","  \n","  label = (img_path.split(\"/\")[-2])#contains the true label\n","  print(label)\n","  break\n","\n","image.flatten()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qfnt7VqXjZYx","executionInfo":{"status":"ok","timestamp":1656694620100,"user_tz":240,"elapsed":610,"user":{"displayName":"Venetis Pallikaras","userId":"14147789031826417236"}},"outputId":"d71560d2-e87e-42ec-b173-e4b2a6beb259"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["apple_golden_2\n"]},{"output_type":"execute_result","data":{"text/plain":["array([255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 246, 246, 240,\n","       222, 221, 196, 205, 202, 158, 197, 193, 137, 189, 185, 122, 187,\n","       182, 115, 188, 182, 114, 183, 177, 109, 179, 173, 107, 171, 165,\n","       106, 172, 167, 109, 191, 186, 135, 191, 185, 138, 169, 162, 109,\n","       169, 163, 110, 203, 198, 153, 207, 201, 157, 206, 202, 160, 211,\n","       207, 174, 209, 204, 177, 215, 213, 190, 225, 225, 210, 236, 237,\n","       229, 249, 249, 248, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 240, 239, 229, 212, 208, 168, 195, 188,\n","       121, 190, 183, 103, 193, 187,  99, 195, 188, 100, 197, 190, 103,\n","       198, 191, 103, 197, 191, 100, 196, 190,  99, 192, 185,  94, 189,\n","       182,  93, 186, 179,  91, 185, 177,  88, 185, 177,  87, 183, 176,\n","        90, 182, 175,  88, 186, 176,  90, 186, 177,  92, 184, 175,  89,\n","       177, 169,  84, 169, 159,  78, 165, 156,  77, 162, 155,  78, 163,\n","       156,  88, 167, 159, 106, 179, 173, 135, 209, 207, 190, 243, 243,\n","       241, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       246, 246, 241, 215, 209, 171, 196, 185, 112, 195, 181,  92, 202,\n","       189,  99, 203, 195, 105, 206, 200, 108, 206, 200, 108, 203, 198,\n","       105, 205, 199, 108, 205, 198, 105, 205, 196, 102, 203, 195, 103,\n","       201, 194, 100, 199, 192,  96, 199, 191,  96, 199, 191,  96, 196,\n","       189,  97, 195, 188,  97, 194, 187,  92, 195, 187,  95, 196, 190,\n","        98, 192, 185,  94, 185, 178,  87, 178, 169,  80, 169, 160,  73,\n","       168, 157,  73, 164, 154,  72, 152, 143,  63, 143, 135,  62, 144,\n","       140,  86, 185, 185, 160, 243, 243, 241, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 242, 241,\n","       232, 203, 193, 137, 199, 182,  98, 201, 186,  99, 208, 194, 102,\n","       212, 200, 108, 214, 208, 115, 218, 212, 116, 210, 204, 109, 206,\n","       199, 108, 215, 208, 117, 216, 206, 113, 211, 201, 106, 203, 194,\n","       100, 206, 197, 103, 206, 199, 102, 207, 199, 104, 206, 197, 104,\n","       202, 195, 102, 202, 196, 102, 199, 190,  94, 200, 192,  98, 202,\n","       195, 103, 201, 193,  98, 196, 187,  92, 190, 181,  88, 181, 173,\n","        85, 171, 162,  75, 165, 155,  71, 161, 154,  70, 152, 147,  64,\n","       145, 138,  62, 135, 127,  59, 141, 139,  90, 212, 212, 203, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 231, 227, 210, 201,\n","       187, 123, 206, 188, 105, 211, 193, 110, 207, 192, 100, 212, 202,\n","       105, 219, 209, 114, 221, 216, 120, 223, 217, 121, 223, 217, 122,\n","       222, 216, 122, 220, 215, 117, 221, 213, 119, 216, 207, 112, 207,\n","       199, 100, 213, 204, 107, 212, 204, 110, 210, 202, 106, 205, 197,\n","       100, 200, 192,  97, 206, 197, 104, 204, 197, 101, 203, 197, 102,\n","       203, 197, 102, 197, 190,  93, 198, 189,  93, 193, 185,  87, 188,\n","       180,  85, 182, 174,  81, 172, 164,  73, 162, 156,  68, 156, 150,\n","        64, 151, 145,  63, 146, 140,  59, 136, 130,  52, 122, 121,  63,\n","       195, 197, 181, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 230, 228, 210, 196, 183, 118,\n","       209, 191, 112, 212, 194, 111, 213, 198, 112, 217, 201, 110, 218,\n","       207, 113, 223, 215, 122, 227, 220, 124, 228, 222, 127, 228, 221,\n","       128, 227, 220, 128, 226, 220, 127, 224, 217, 126, 220, 213, 119,\n","       221, 211, 115, 216, 208, 114, 213, 205, 112, 212, 204, 109, 209,\n","       202, 105, 206, 198, 102, 208, 200, 104, 207, 200, 104, 202, 198,\n","        99, 202, 197,  99, 196, 189,  92, 196, 188,  92, 195, 189,  92,\n","       190, 182,  87, 183, 176,  80, 175, 168,  75, 164, 158,  66, 157,\n","       150,  60, 153, 145,  58, 147, 141,  58, 141, 135,  55, 133, 127,\n","        56, 114, 113,  59, 199, 200, 188, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 238, 235, 218, 203, 189, 123, 207, 189,\n","       112, 202, 188, 107, 213, 201, 115, 214, 204, 113, 222, 208, 117,\n","       221, 208, 120, 228, 219, 129, 230, 223, 131, 229, 222, 131, 228,\n","       218, 126, 226, 215, 121, 224, 217, 126, 227, 220, 129, 223, 215,\n","       122, 214, 205, 110, 213, 207, 110, 212, 206, 108, 212, 206, 109,\n","       212, 204, 107, 211, 204, 105, 209, 202, 103, 207, 200, 106, 202,\n","       198, 100, 202, 197,  99, 201, 195,  97, 197, 191,  94, 196, 191,\n","        93, 186, 179,  84, 180, 175,  78, 175, 169,  74, 165, 157,  61,\n","       154, 146,  53, 151, 142,  53, 149, 141,  56, 144, 136,  56, 138,\n","       131,  53, 126, 121,  51, 109, 107,  60, 215, 216, 210, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 253, 254, 252, 214, 208, 153, 213, 196, 120, 212,\n","       194, 117, 215, 200, 119, 219, 207, 118, 223, 212, 118, 230, 219,\n","       127, 232, 223, 135, 233, 226, 137, 235, 228, 137, 234, 227, 136,\n","       233, 225, 133, 228, 220, 126, 227, 221, 129, 229, 221, 130, 225,\n","       216, 123, 216, 209, 113, 212, 206, 107, 212, 204, 106, 212, 205,\n","       106, 213, 204, 106, 212, 203, 105, 208, 201, 101, 203, 196, 100,\n","       195, 190,  93, 200, 195,  97, 202, 195,  98, 200, 194,  95, 197,\n","       191,  93, 191, 185,  86, 184, 178,  82, 178, 172,  77, 167, 158,\n","        63, 155, 143,  52, 148, 139,  50, 148, 140,  53, 148, 138,  56,\n","       143, 136,  54, 133, 127,  51, 112, 107,  44, 128, 127,  99, 246,\n","       247, 246, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 232, 230, 204, 211, 199, 128, 218, 197, 123,\n","       218, 201, 122, 223, 210, 123, 226, 214, 121, 232, 221, 128, 235,\n","       228, 137, 237, 230, 139, 238, 231, 141, 238, 230, 142, 237, 230,\n","       140, 235, 229, 137, 234, 228, 136, 231, 226, 136, 228, 221, 130,\n","       225, 217, 123, 220, 214, 117, 214, 208, 110, 210, 201, 104, 214,\n","       204, 105, 214, 203, 105, 213, 203, 105, 209, 200, 101, 201, 193,\n","        96, 188, 183,  84, 198, 192,  94, 203, 195,  95, 201, 194,  95,\n","       196, 190,  92, 193, 187,  89, 186, 180,  83, 178, 171,  76, 165,\n","       156,  62, 159, 148,  55, 150, 139,  51, 145, 132,  49, 144, 132,\n","        49, 146, 138,  54, 138, 130,  50, 122, 118,  47,  98,  95,  45,\n","       183, 183, 170, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 247, 246, 238, 212, 201, 140, 218, 201, 126, 222, 204,\n","       125, 225, 208, 125, 227, 214, 125, 231, 222, 131, 237, 228, 137,\n","       237, 230, 139, 238, 232, 140, 239, 233, 145, 238, 231, 144, 237,\n","       230, 140, 236, 231, 140, 236, 230, 144, 234, 227, 140, 228, 220,\n","       128, 220, 213, 117, 220, 213, 117, 217, 211, 114, 216, 209, 113,\n","       214, 206, 110, 213, 204, 106, 211, 202, 104, 209, 199, 101, 202,\n","       194,  92, 200, 192,  95, 202, 194,  98, 204, 196,  97, 202, 193,\n","        95, 196, 188,  89, 191, 186,  86, 187, 181,  84, 180, 173,  76,\n","       170, 163,  65, 163, 154,  60, 155, 144,  54, 147, 136,  51, 142,\n","       132,  49, 145, 135,  54, 138, 130,  50, 126, 121,  46, 107, 102,\n","        42, 114, 112,  82, 238, 238, 237, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 230, 226, 192, 214, 199, 128, 222, 206, 129, 226,\n","       208, 124, 229, 212, 125, 234, 220, 134, 234, 227, 137, 237, 231,\n","       139, 239, 232, 143, 241, 233, 146, 241, 235, 148, 240, 232, 145,\n","       239, 232, 145, 239, 234, 148, 240, 235, 151, 235, 231, 145, 232,\n","       227, 138, 228, 222, 131, 225, 220, 128, 221, 215, 121, 220, 214,\n","       120, 218, 211, 118, 214, 207, 111, 207, 200, 103, 210, 202, 103,\n","       205, 198,  97, 202, 195,  97, 203, 195, 101, 206, 198, 100, 203,\n","       194,  96, 196, 187,  88, 192, 185,  84, 188, 181,  82, 181, 174,\n","        77, 172, 167,  69, 165, 159,  66, 157, 150,  60, 151, 144,  55,\n","       146, 139,  53, 142, 132,  49, 135, 128,  47, 128, 121,  44, 112,\n","       108,  44,  93,  89,  46, 183, 182, 174, 255, 255, 255, 255, 255,\n","       255, 249, 249, 242, 216, 205, 144, 219, 206, 129, 225, 208, 128,\n","       230, 214, 127, 234, 221, 131, 237, 227, 137, 237, 229, 139, 239,\n","       231, 143, 242, 234, 146, 243, 236, 148, 244, 238, 150, 242, 235,\n","       148, 245, 237, 155, 245, 239, 159, 242, 237, 154, 240, 235, 152,\n","       239, 234, 151, 233, 228, 144, 228, 223, 134, 223, 217, 127, 223,\n","       217, 123, 220, 213, 120, 217, 209, 116, 213, 205, 110, 213, 205,\n","       111, 211, 205, 109, 207, 202, 104, 205, 200, 102, 204, 198, 101,\n","       202, 196,  98, 198, 192,  93, 193, 188,  89, 186, 181,  82, 178,\n","       173,  74, 170, 164,  69, 165, 158,  65, 157, 150,  58, 150, 144,\n","        53, 144, 137,  49, 139, 131,  46, 134, 125,  44, 127, 120,  40,\n","       113, 108,  40,  95,  89,  40, 130, 129, 112, 246, 247, 247, 255,\n","       255, 255, 242, 239, 220, 215, 202, 130, 223, 209, 129, 228, 214,\n","       129, 235, 221, 131, 235, 226, 134, 237, 230, 137, 239, 231, 143,\n","       240, 233, 148, 243, 236, 149, 243, 236, 147, 247, 239, 154, 251,\n","       243, 164, 252, 245, 173, 248, 243, 169, 246, 240, 164, 243, 238,\n","       160, 240, 236, 154, 236, 229, 147, 230, 224, 139, 226, 221, 132,\n","       225, 218, 129, 222, 215, 123, 218, 211, 120, 216, 208, 116, 215,\n","       207, 114, 213, 207, 111, 209, 205, 107, 206, 202, 104, 204, 198,\n","       102, 203, 197,  99, 198, 192,  96, 192, 188,  90, 184, 180,  81,\n","       175, 170,  72, 168, 160,  67, 165, 157,  64, 157, 151,  56, 150,\n","       144,  50, 143, 137,  47, 137, 131,  45, 132, 121,  39, 127, 118,\n","        36, 114, 109,  36,  98,  92,  36,  99,  96,  66, 210, 209, 207,\n","       255, 255, 255, 231, 226, 189, 218, 207, 130, 223, 212, 130, 230,\n","       221, 133, 231, 223, 133, 236, 228, 138, 238, 232, 139, 240, 234,\n","       143, 240, 235, 146, 241, 235, 147, 240, 234, 147, 248, 241, 160,\n","       250, 247, 172, 253, 251, 180, 245, 241, 166, 246, 242, 173, 246,\n","       243, 172, 243, 239, 162, 239, 234, 153, 235, 228, 146, 228, 222,\n","       135, 226, 219, 133, 224, 217, 130, 212, 206, 116, 212, 205, 114,\n","       216, 207, 114, 215, 206, 113, 211, 205, 109, 208, 202, 106, 205,\n","       200, 102, 203, 197,  99, 198, 191,  96, 191, 185,  89, 185, 179,\n","        83, 176, 171,  72, 169, 162,  64, 163, 157,  61, 156, 150,  55,\n","       145, 139,  46, 141, 135,  47, 136, 130,  44, 127, 118,  35, 128,\n","       120,  40, 121, 114,  40, 105,  98,  35,  90,  85,  46, 177, 176,\n","       169, 250, 250, 246, 218, 212, 152, 219, 209, 131, 224, 215, 131,\n","       230, 223, 133, 226, 219, 128, 233, 227, 136, 237, 232, 141, 241,\n","       236, 144, 240, 236, 147, 242, 237, 149, 244, 237, 152, 248, 240,\n","       160, 252, 248, 174, 254, 253, 183, 251, 249, 180, 250, 249, 185,\n","       252, 250, 186, 248, 245, 177, 242, 240, 164, 237, 232, 150, 230,\n","       223, 137, 225, 218, 132, 224, 217, 132, 219, 212, 122, 216, 209,\n","       117, 215, 206, 113, 214, 206, 112, 210, 206, 108, 207, 203, 105,\n","       204, 200, 101, 202, 196,  98, 196, 190,  92, 190, 184,  87, 184,\n","       178,  83, 176, 169,  71, 167, 160,  62, 162, 155,  59, 154, 148,\n","        54, 147, 142,  49, 141, 135,  47, 136, 130,  45, 131, 123,  40,\n","       130, 122,  41, 122, 115,  39, 106,  99,  36,  87,  83,  39, 147,\n","       147, 133, 239, 239, 219, 214, 206, 136, 222, 211, 134, 225, 217,\n","       131, 229, 223, 132, 232, 225, 135, 232, 225, 132, 236, 231, 139,\n","       241, 235, 145, 242, 236, 147, 243, 237, 150, 244, 238, 152, 247,\n","       240, 157, 251, 247, 173, 253, 252, 187, 252, 252, 188, 252, 252,\n","       191, 254, 254, 192, 252, 249, 184, 248, 242, 169, 239, 235, 154,\n","       230, 226, 141, 227, 222, 136, 225, 218, 133, 222, 215, 127, 216,\n","       210, 117, 210, 204, 111, 212, 206, 111, 212, 204, 110, 208, 201,\n","       106, 204, 198, 101, 200, 194,  97, 195, 189,  88, 189, 184,  84,\n","       183, 178,  78, 173, 167,  69, 163, 157,  60, 159, 153,  59, 152,\n","       147,  55, 146, 141,  48, 138, 132,  43, 136, 128,  46, 132, 124,\n","        42, 130, 121,  42, 121, 114,  38, 104,  99,  35,  88,  82,  38,\n","       119, 118, 104, 231, 227, 192, 217, 206, 131, 222, 211, 133, 226,\n","       219, 132, 230, 224, 134, 234, 227, 136, 235, 229, 136, 238, 233,\n","       141, 241, 235, 147, 240, 235, 146, 241, 234, 147, 243, 237, 151,\n","       246, 240, 157, 252, 246, 172, 252, 250, 182, 252, 251, 185, 253,\n","       251, 189, 252, 251, 187, 250, 247, 177, 241, 234, 159, 238, 233,\n","       153, 232, 227, 142, 227, 223, 136, 224, 217, 130, 220, 213, 125,\n","       215, 209, 115, 212, 206, 111, 211, 205, 110, 210, 203, 109, 207,\n","       200, 105, 203, 197, 100, 200, 194,  96, 194, 189,  88, 187, 182,\n","        81, 176, 171,  71, 171, 165,  67, 166, 159,  64, 159, 152,  60,\n","       152, 147,  55, 145, 140,  47, 138, 133,  42, 135, 129,  44, 132,\n","       124,  40, 128, 119,  40, 122, 114,  38, 106, 101,  36,  89,  83,\n","        39,  99,  99,  81, 219, 213, 163, 219, 206, 129, 221, 212, 131,\n","       226, 218, 131, 229, 223, 132, 232, 226, 133, 235, 230, 136, 234,\n","       228, 135, 233, 227, 138, 237, 233, 144, 238, 231, 143, 243, 235,\n","       148, 244, 238, 154, 248, 242, 165, 249, 244, 171, 250, 246, 174,\n","       250, 246, 175, 248, 244, 170, 244, 241, 164, 243, 237, 158, 240,\n","       234, 154, 233, 227, 142, 226, 221, 134, 223, 215, 126, 218, 211,\n","       121, 215, 208, 114, 212, 207, 110, 210, 204, 108, 206, 200, 104,\n","       202, 197,  99, 201, 197,  99, 197, 193,  95, 193, 188,  90, 186,\n","       180,  82, 178, 171,  74, 173, 165,  70, 168, 158,  65, 157, 149,\n","        56, 150, 144,  51, 146, 139,  49, 139, 133,  45, 135, 129,  44,\n","       132, 124,  41, 128, 120,  39, 122, 115,  37, 107, 101,  36,  87,\n","        82,  36,  98,  98,  78, 204, 195, 137, 217, 205, 128, 222, 212,\n","       130, 225, 219, 132, 228, 222, 131, 229, 223, 129, 235, 228, 135,\n","       235, 229, 133, 233, 227, 134, 238, 232, 140, 240, 233, 144, 241,\n","       234, 145, 241, 236, 149, 243, 238, 156, 245, 242, 164, 245, 242,\n","       165, 243, 241, 161, 241, 238, 156, 239, 236, 153, 241, 236, 154,\n","       238, 231, 148, 231, 225, 139, 227, 220, 134, 221, 214, 124, 217,\n","       210, 118, 213, 206, 111, 210, 205, 108, 208, 202, 106, 206, 200,\n","       103, 203, 197, 101, 199, 194,  96, 190, 186,  87, 189, 185,  87,\n","       186, 180,  84, 181, 173,  80, 174, 166,  70, 167, 158,  63, 156,\n","       148,  54, 150, 143,  51, 146, 138,  50, 139, 132,  46, 136, 128,\n","        44, 132, 124,  41, 127, 119,  39, 121, 113,  38, 106,  99,  35,\n","        88,  82,  35,  98,  99,  77, 206, 197, 135, 215, 206, 127, 222,\n","       212, 130, 225, 219, 132, 225, 220, 127, 226, 221, 125, 233, 228,\n","       130, 234, 229, 130, 236, 230, 133, 238, 231, 137, 238, 232, 140,\n","       239, 232, 141, 238, 233, 142, 236, 231, 143, 242, 237, 155, 242,\n","       238, 155, 240, 236, 151, 238, 234, 149, 235, 230, 146, 236, 231,\n","       147, 233, 227, 142, 226, 220, 134, 224, 217, 130, 220, 213, 121,\n","       215, 209, 114, 210, 204, 107, 207, 201, 105, 205, 199, 103, 204,\n","       198, 102, 203, 196, 101, 198, 192,  95, 192, 189,  89, 188, 183,\n","        85, 184, 178,  80, 179, 172,  77, 174, 166,  70, 166, 157,  62,\n","       156, 147,  53, 152, 144,  52, 146, 138,  49, 139, 131,  46, 135,\n","       124,  42, 129, 119,  38, 125, 116,  39, 119, 110,  38, 104,  97,\n","        35,  88,  81,  35,  97,  98,  75, 211, 201, 137, 216, 204, 128,\n","       222, 211, 129, 225, 217, 127, 226, 221, 124, 228, 224, 125, 232,\n","       227, 130, 235, 230, 132, 237, 230, 134, 237, 229, 136, 237, 231,\n","       137, 237, 230, 136, 231, 225, 132, 231, 225, 134, 238, 233, 147,\n","       238, 233, 149, 238, 233, 147, 235, 230, 143, 234, 228, 142, 233,\n","       227, 141, 230, 224, 138, 224, 219, 130, 219, 213, 123, 216, 210,\n","       115, 213, 208, 112, 210, 205, 108, 203, 199, 101, 201, 197,  98,\n","       203, 197, 101, 203, 197,  99, 199, 192,  96, 192, 189,  89, 187,\n","       181,  81, 183, 175,  76, 179, 170,  74, 172, 163,  66, 165, 156,\n","        60, 156, 149,  55, 153, 144,  52, 145, 139,  46, 139, 131,  43,\n","       133, 123,  41, 123, 115,  36, 122, 111,  37, 118, 108,  38, 102,\n","        95,  35,  85,  79,  37,  99,  98,  79, 209, 200, 136, 215, 204,\n","       128, 220, 211, 128, 221, 215, 123, 225, 219, 122, 227, 222, 123,\n","       229, 223, 124, 232, 227, 129, 236, 230, 134, 238, 229, 135, 236,\n","       230, 133, 236, 230, 134, 235, 228, 134, 234, 228, 133, 235, 229,\n","       139, 236, 229, 144, 235, 229, 142, 233, 226, 136, 230, 224, 134,\n","       227, 221, 133, 225, 219, 128, 221, 215, 122, 216, 211, 116, 215,\n","       209, 113, 212, 206, 110, 209, 204, 107, 206, 203, 103, 203, 199,\n","       100, 203, 197, 101, 202, 195,  99, 198, 191,  94, 189, 185,  85,\n","       181, 175,  75, 180, 172,  73, 177, 166,  68, 171, 160,  62, 163,\n","       156,  57, 158, 151,  56, 153, 145,  52, 145, 138,  46, 137, 130,\n","        42, 127, 118,  37, 116, 108,  30, 118, 108,  35, 118, 108,  40,\n","       100,  93,  36,  82,  77,  39, 103, 102,  87, 206, 199, 133, 214,\n","       205, 130, 217, 210, 126, 218, 212, 120, 223, 217, 121, 226, 220,\n","       122, 228, 219, 121, 231, 225, 127, 234, 229, 131, 235, 230, 132,\n","       236, 229, 133, 237, 228, 136, 236, 229, 133, 235, 229, 132, 235,\n","       229, 134, 230, 224, 134, 231, 224, 134, 229, 222, 129, 225, 219,\n","       123, 223, 217, 122, 221, 215, 122, 219, 213, 116, 214, 210, 111,\n","       212, 208, 108, 206, 202, 104, 204, 200, 101, 204, 200, 101, 203,\n","       199, 100, 202, 197,  99, 200, 194,  97, 196, 190,  92, 190, 185,\n","        86, 183, 176,  76, 180, 170,  71, 176, 164,  66, 169, 158,  60,\n","       163, 155,  57, 159, 150,  53, 153, 143,  48, 144, 136,  43, 133,\n","       125,  36, 127, 118,  36, 114, 105,  31, 114, 102,  32, 115, 106,\n","        37,  97,  91,  36,  80,  75,  39, 115, 115,  99, 207, 202, 143,\n","       213, 203, 128, 216, 209, 125, 218, 212, 121, 221, 215, 120, 225,\n","       220, 121, 230, 223, 124, 230, 225, 126, 232, 226, 129, 234, 227,\n","       131, 233, 227, 131, 236, 228, 135, 236, 229, 132, 233, 229, 130,\n","       233, 228, 131, 231, 225, 130, 229, 224, 130, 226, 220, 127, 221,\n","       216, 120, 219, 212, 116, 217, 211, 115, 215, 210, 111, 213, 208,\n","       110, 210, 205, 107, 206, 201, 102, 203, 198,  99, 203, 198,  99,\n","       203, 198,  99, 200, 195,  96, 198, 192,  95, 197, 189,  94, 193,\n","       184,  89, 186, 178,  78, 179, 169,  69, 174, 163,  64, 169, 159,\n","        63, 163, 154,  56, 156, 148,  52, 150, 142,  50, 141, 135,  42,\n","       132, 125,  36, 128, 118,  35, 118, 109,  32, 116, 105,  34, 110,\n","       101,  35,  93,  87,  34,  76,  74,  39, 129, 131, 117, 213, 209,\n","       160, 211, 201, 126, 216, 207, 124, 218, 209, 122, 220, 214, 120,\n","       225, 218, 118, 229, 221, 122, 231, 223, 124, 231, 224, 125, 227,\n","       222, 124, 229, 225, 126, 232, 227, 130, 232, 228, 130, 231, 226,\n","       127, 230, 225, 125, 229, 224, 126, 227, 222, 124, 224, 218, 121,\n","       220, 214, 116, 217, 209, 110, 214, 207, 108, 214, 207, 108, 211,\n","       205, 105, 210, 203, 103, 206, 199, 100, 202, 198,  97, 203, 197,\n","        98, 203, 196,  96, 197, 192,  94, 196, 190,  93, 194, 187,  88,\n","       189, 182,  84, 184, 178,  78, 175, 169,  68, 171, 163,  66, 167,\n","       158,  62, 162, 153,  55, 154, 146,  51, 147, 140,  49, 140, 133,\n","        44, 132, 124,  38, 127, 117,  35, 120, 111,  32, 116, 105,  32,\n","       104,  96,  30,  88,  82,  31,  77,  74,  43, 150, 152, 142, 221,\n","       219, 181, 208, 199, 125, 214, 205, 125, 216, 209, 119, 221, 214,\n","       117, 225, 217, 118, 229, 221, 122, 232, 223, 124, 230, 224, 123,\n","       227, 222, 121, 229, 223, 124, 230, 224, 125, 230, 225, 127, 229,\n","       226, 127, 228, 224, 124, 226, 222, 123, 225, 219, 120, 223, 216,\n","       119, 219, 213, 114, 216, 209, 109, 213, 208, 105, 212, 206, 104,\n","       211, 203, 103, 207, 200, 100, 202, 197,  98, 200, 196,  95, 201,\n","       196,  97, 202, 194,  95, 198, 190,  94, 194, 188,  90, 189, 183,\n","        85, 185, 179,  81, 180, 175,  76, 175, 168,  70, 169, 161,  66,\n","       163, 156,  58, 156, 148,  52, 149, 140,  47, 144, 136,  46, 140,\n","       130,  44, 131, 121,  37, 124, 114,  32, 117, 109,  30, 111, 101,\n","        30, 100,  91,  30,  84,  77,  30,  79,  76,  50, 175, 177, 172,\n","       226, 224, 194, 205, 196, 126, 213, 204, 126, 216, 209, 119, 220,\n","       213, 116, 221, 213, 114, 227, 218, 118, 230, 222, 119, 231, 224,\n","       120, 228, 221, 120, 227, 218, 119, 231, 223, 124, 232, 224, 125,\n","       229, 224, 124, 227, 223, 123, 225, 220, 120, 221, 215, 115, 220,\n","       214, 116, 218, 211, 113, 216, 208, 108, 211, 206, 104, 206, 202,\n","       100, 207, 199, 101, 203, 198,  98, 202, 197,  97, 202, 195,  96,\n","       203, 195,  96, 200, 192,  94, 197, 190,  91, 192, 186,  87, 187,\n","       181,  82, 184, 178,  79, 178, 172,  73, 171, 166,  67, 165, 158,\n","        60, 159, 152,  55, 154, 146,  52, 148, 140,  47, 140, 133,  43,\n","       137, 128,  41, 130, 120,  37, 120, 111,  32, 114, 105,  32, 108,\n","        98,  30,  97,  87,  30,  81,  73,  30,  85,  81,  60, 204, 205,\n","       203, 238, 237, 220, 207, 197, 132, 210, 201, 123, 214, 205, 120,\n","       219, 210, 119, 221, 213, 114, 225, 217, 116, 227, 219, 117, 228,\n","       221, 117, 229, 222, 119, 229, 221, 122, 230, 222, 123, 230, 223,\n","       124, 229, 223, 123, 227, 220, 120, 226, 218, 118, 221, 214, 117,\n","       218, 212, 115, 217, 210, 112, 214, 206, 107, 209, 203, 102, 205,\n","       200,  99, 203, 199,  99, 202, 198,  99, 204, 197,  98, 203, 195,\n","        96, 202, 194,  95, 198, 191,  91, 194, 188,  89, 190, 183,  85,\n","       188, 180,  81, 182, 176,  76, 176, 168,  69, 169, 163,  64, 163,\n","       158,  60, 158, 150,  56, 154, 145,  52, 147, 138,  45, 139, 132,\n","        41, 134, 126,  37, 126, 118,  34, 120, 111,  33, 113, 103,  33,\n","       104,  94,  29,  91,  82,  29,  77,  70,  32,  95,  93,  76, 228,\n","       229, 228, 248, 248, 242, 209, 201, 142, 208, 200, 119, 214, 204,\n","       121, 219, 209, 121, 224, 215, 118, 224, 216, 115, 224, 217, 114,\n","       225, 218, 114, 226, 219, 116, 228, 221, 121, 227, 220, 120, 228,\n","       220, 121, 229, 221, 121, 226, 219, 118, 222, 216, 115, 221, 214,\n","       115, 220, 212, 113, 217, 208, 110, 213, 205, 106, 210, 202, 103,\n","       205, 198,  97, 203, 199,  99, 202, 198,  99, 204, 196,  97, 203,\n","       195,  96, 199, 192,  93, 195, 191,  90, 192, 185,  85, 190, 182,\n","        83, 188, 179,  80, 180, 172,  73, 173, 166,  67, 169, 162,  65,\n","       160, 154,  59, 155, 147,  52, 150, 141,  49, 143, 135,  43, 136,\n","       129,  39, 131, 123,  36, 125, 115,  33, 119, 107,  31, 111,  99,\n","        30,  99,  89,  28,  85,  77,  28,  74,  68,  36, 111, 110,  98,\n","       247, 247, 247, 254, 255, 254, 212, 208, 158, 207, 198, 121, 212,\n","       202, 120, 218, 208, 119, 222, 213, 117, 222, 213, 114, 219, 210,\n","       108, 223, 214, 110, 226, 217, 114, 226, 220, 117, 226, 220, 118,\n","       226, 219, 117, 228, 219, 119, 225, 217, 116, 220, 214, 113, 221,\n","       213, 114, 219, 210, 111, 215, 207, 108, 210, 204, 103, 206, 200,\n","       100, 204, 196,  97, 202, 197,  97, 202, 197,  97, 201, 195,  94,\n","       200, 192,  93, 197, 191,  90, 196, 190,  88, 192, 184,  84, 188,\n","       180,  81, 184, 176,  77, 178, 171,  73, 172, 166,  69, 166, 160,\n","        63, 159, 151,  54, 151, 142,  47, 145, 137,  45, 141, 133,  44,\n","       135, 127,  38, 130, 120,  35, 123, 112,  31, 116, 105,  31, 109,\n","        96,  30,  93,  84,  28,  80,  72,  28,  73,  65,  37, 139, 136,\n","       129, 255, 255, 255, 255, 255, 255, 222, 219, 185, 204, 194, 121,\n","       210, 202, 121, 215, 206, 120, 216, 207, 112, 222, 211, 111, 223,\n","       211, 111, 224, 213, 110, 226, 215, 111, 226, 217, 115, 225, 219,\n","       117, 225, 219, 117, 227, 217, 118, 225, 217, 117, 222, 214, 114,\n","       220, 212, 113, 218, 209, 110, 214, 206, 107, 209, 203, 101, 206,\n","       200, 100, 202, 194,  95, 202, 194,  95, 201, 193,  94, 197, 191,\n","        92, 197, 191,  90, 197, 189,  89, 195, 186,  86, 191, 183,  84,\n","       186, 178,  79, 180, 172,  74, 175, 169,  70, 171, 163,  67, 163,\n","       157,  60, 156, 148,  51, 148, 140,  44, 144, 136,  43, 139, 131,\n","        41, 134, 124,  37, 127, 117,  32, 121, 110,  30, 113, 103,  30,\n","       103,  93,  28,  88,  80,  28,  77,  69,  29,  70,  65,  42, 174,\n","       174, 170, 255, 255, 255, 255, 255, 255, 240, 239, 226, 201, 194,\n","       125, 207, 200, 118, 212, 203, 119, 217, 207, 116, 221, 210, 111,\n","       223, 210, 110, 224, 212, 111, 226, 214, 112, 227, 215, 113, 226,\n","       217, 115, 226, 218, 115, 225, 213, 112, 223, 213, 113, 222, 212,\n","       114, 220, 210, 112, 217, 207, 108, 214, 206, 105, 211, 203, 102,\n","       207, 199, 100, 205, 196,  98, 203, 195,  96, 200, 192,  93, 196,\n","       189,  87, 195, 189,  87, 199, 189,  90, 195, 185,  87, 188, 181,\n","        82, 183, 175,  75, 178, 170,  70, 174, 164,  66, 168, 160,  62,\n","       161, 153,  58, 153, 145,  51, 146, 138,  41, 140, 132,  38, 135,\n","       128,  38, 132, 121,  36, 126, 115,  31, 120, 109,  31, 109,  99,\n","        28,  97,  87,  25,  84,  75,  25,  74,  67,  32,  80,  77,  60,\n","       215, 216, 213, 255, 255, 255, 255, 255, 255, 251, 251, 249, 205,\n","       199, 143, 204, 195, 116, 210, 200, 118, 215, 204, 114, 221, 209,\n","       111, 220, 208, 108, 221, 209, 107, 225, 213, 112, 225, 213, 111,\n","       227, 215, 112, 227, 215, 113, 226, 213, 113, 225, 213, 114, 220,\n","       210, 111, 216, 208, 108, 216, 207, 106, 214, 205, 104, 209, 201,\n","       100, 206, 198,  98, 204, 195,  96, 201, 194,  94, 199, 192,  91,\n","       197, 192,  89, 196, 191,  89, 198, 189,  90, 193, 185,  86, 187,\n","       179,  80, 181, 174,  73, 175, 167,  67, 171, 163,  64, 165, 156,\n","        59, 158, 149,  55, 150, 143,  48, 143, 135,  40, 138, 129,  39,\n","       134, 126,  38, 130, 121,  37, 123, 113,  32, 116, 106,  30, 106,\n","        95,  28,  95,  84,  26,  80,  71,  23,  71,  65,  35, 113, 114,\n","       102, 248, 249, 248, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       216, 212, 172, 201, 192, 116, 210, 200, 119, 216, 204, 115, 220,\n","       207, 111, 219, 207, 109, 222, 210, 109, 223, 211, 109, 221, 209,\n","       107, 225, 213, 111, 225, 213, 111, 222, 210, 109, 221, 209, 111,\n","       216, 206, 106, 212, 205, 102, 214, 206, 105, 212, 204, 103, 208,\n","       200,  99, 204, 196,  95, 198, 192,  90, 198, 193,  91, 197, 192,\n","        90, 199, 190,  89, 196, 188,  88, 195, 187,  87, 189, 183,  82,\n","       184, 177,  78, 179, 171,  72, 174, 165,  67, 167, 160,  62, 160,\n","       152,  58, 155, 146,  51, 149, 140,  45, 143, 133,  43, 138, 128,\n","        41, 134, 125,  38, 127, 118,  33, 120, 110,  30, 113, 102,  29,\n","       103,  91,  28,  90,  80,  28,  77,  68,  26,  76,  71,  45, 174,\n","       174, 168, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 238, 237, 220, 198, 192, 121, 204, 195, 117, 211, 200, 112,\n","       218, 205, 110, 217, 204, 108, 220, 208, 108, 221, 209, 107, 220,\n","       208, 106, 222, 210, 109, 223, 211, 110, 220, 208, 107, 218, 206,\n","       105, 219, 207, 107, 215, 206, 105, 211, 202, 101, 210, 201, 100,\n","       207, 199,  97, 202, 194,  92, 198, 191,  89, 198, 191,  89, 198,\n","       190,  88, 197, 186,  86, 193, 184,  83, 191, 184,  83, 187, 179,\n","        80, 182, 174,  76, 174, 167,  71, 170, 162,  67, 161, 156,  58,\n","       154, 149,  52, 150, 143,  50, 144, 136,  44, 136, 128,  39, 132,\n","       124,  36, 129, 121,  34, 124, 114,  30, 117, 106,  28, 108,  98,\n","        27,  98,  87,  27,  83,  74,  26,  73,  67,  33,  96,  97,  80,\n","       226, 227, 226, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 249, 248, 244, 199, 196, 137, 202, 193, 119, 208, 196,\n","       113, 212, 198, 108, 216, 203, 108, 216, 204, 104, 219, 207, 105,\n","       216, 204, 102, 218, 206, 105, 219, 207, 106, 218, 206, 105, 220,\n","       208, 106, 219, 206, 105, 215, 204, 101, 210, 198,  97, 209, 197,\n","        97, 207, 196,  95, 203, 191,  91, 197, 187,  86, 196, 187,  84,\n","       195, 185,  83, 190, 182,  80, 189, 182,  80, 188, 179,  79, 184,\n","       174,  76, 177, 170,  72, 171, 163,  69, 166, 158,  64, 159, 153,\n","        55, 153, 147,  51, 147, 140,  49, 140, 133,  42, 133, 126,  37,\n","       130, 123,  35, 126, 118,  32, 119, 109,  27, 111, 101,  27, 102,\n","        93,  27,  92,  82,  26,  78,  69,  23,  76,  72,  43, 142, 142,\n","       133, 254, 253, 254, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 216, 214, 179, 198, 190, 115, 204,\n","       192, 115, 208, 195, 109, 211, 198, 107, 213, 200, 103, 216, 203,\n","       103, 214, 202, 100, 214, 201,  99, 215, 203, 101, 216, 204, 103,\n","       217, 205, 106, 216, 204, 104, 214, 202, 100, 210, 198,  96, 208,\n","       196,  95, 204, 193,  93, 198, 188,  87, 191, 181,  79, 191, 182,\n","        79, 192, 181,  80, 191, 179,  78, 186, 177,  75, 183, 175,  75,\n","       179, 170,  71, 173, 166,  65, 170, 160,  62, 165, 155,  60, 155,\n","       149,  52, 151, 143,  49, 146, 138,  46, 138, 131,  40, 133, 125,\n","        36, 130, 121,  35, 124, 114,  32, 116, 106,  27, 106,  97,  27,\n","        97,  89,  25,  86,  77,  24,  76,  68,  28,  94,  92,  73, 207,\n","       209, 206, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 241, 241, 232, 195, 189, 129,\n","       200, 190, 114, 204, 193, 110, 207, 193, 107, 209, 197, 103, 210,\n","       197,  99, 210, 198,  96, 209, 197,  95, 211, 199,  97, 212, 200,\n","        98, 212, 201,  99, 212, 201,  98, 210, 198,  96, 207, 195,  93,\n","       207, 194,  93, 201, 190,  90, 192, 183,  82, 190, 179,  77, 189,\n","       179,  77, 190, 179,  79, 186, 175,  75, 182, 174,  72, 179, 170,\n","        71, 175, 166,  67, 171, 163,  62, 167, 158,  58, 162, 153,  57,\n","       154, 147,  50, 149, 141,  46, 144, 135,  43, 138, 130,  40, 135,\n","       125,  38, 129, 119,  35, 121, 110,  30, 113, 102,  27, 102,  93,\n","        26,  93,  84,  25,  82,  71,  25,  81,  73,  42, 138, 138, 127,\n","       252, 252, 252, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 212, 210,\n","       177, 194, 185, 116, 199, 189, 110, 201, 187, 102, 204, 192, 100,\n","       205, 194,  96, 206, 194,  95, 208, 196,  94, 209, 196,  94, 209,\n","       196,  94, 206, 193,  92, 207, 195,  93, 207, 195,  92, 204, 192,\n","        92, 203, 191,  91, 198, 187,  87, 192, 180,  79, 189, 177,  75,\n","       184, 175,  73, 185, 177,  76, 182, 175,  73, 182, 172,  72, 178,\n","       168,  69, 173, 164,  66, 169, 160,  62, 163, 154,  59, 157, 150,\n","        54, 153, 145,  50, 146, 138,  46, 141, 132,  42, 135, 126,  38,\n","       130, 121,  35, 125, 114,  32, 117, 105,  29, 108,  96,  27,  97,\n","        86,  25,  88,  76,  24,  79,  70,  33,  99,  97,  75, 208, 208,\n","       204, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 242,\n","       242, 234, 193, 186, 128, 198, 186, 112, 196, 183, 102, 200, 187,\n","        98, 204, 193,  98, 205, 193,  94, 206, 193,  93, 206, 193,  93,\n","       206, 193,  90, 204, 192,  91, 204, 192,  92, 203, 191,  89, 201,\n","       189,  89, 200, 188,  88, 196, 184,  84, 189, 177,  77, 186, 174,\n","        74, 182, 173,  71, 181, 175,  71, 181, 172,  73, 180, 168,  71,\n","       174, 164,  65, 168, 158,  60, 164, 155,  57, 159, 149,  55, 153,\n","       145,  51, 148, 140,  48, 142, 134,  46, 138, 128,  42, 133, 122,\n","        39, 126, 116,  34, 120, 109,  31, 111,  99,  28, 103,  91,  25,\n","        92,  81,  25,  81,  70,  25,  87,  84,  54, 148, 151, 142, 252,\n","       253, 252, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 216, 214, 188, 188, 178, 112, 192, 182, 108, 196,\n","       185, 103, 199, 186,  97, 199, 185,  91, 199, 185,  90, 201, 186,\n","        90, 199, 184,  84, 199, 185,  84, 199, 186,  87, 199, 187,  85,\n","       198, 186,  84, 195, 183,  83, 192, 180,  80, 187, 175,  74, 184,\n","       171,  72, 182, 170,  73, 178, 167,  68, 175, 165,  65, 172, 162,\n","        65, 167, 158,  60, 162, 153,  55, 158, 150,  51, 152, 143,  50,\n","       146, 139,  47, 141, 132,  43, 136, 126,  39, 132, 121,  37, 127,\n","       116,  36, 121, 110,  34, 115, 103,  31, 105,  93,  28,  97,  86,\n","        27,  84,  75,  22,  84,  78,  42, 118, 117,  99, 231, 232, 230,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 251, 251, 251, 201, 198, 159, 184, 174, 105,\n","       193, 181, 106, 195, 182, 100, 194, 180,  91, 194, 180,  87, 196,\n","       181,  86, 193, 178,  80, 193, 179,  79, 192, 179,  80, 193, 180,\n","        81, 191, 179,  78, 189, 177,  78, 187, 175,  74, 183, 171,  70,\n","       176, 164,  65, 174, 161,  64, 172, 159,  62, 167, 157,  60, 163,\n","       154,  58, 158, 149,  52, 154, 145,  49, 150, 142,  45, 147, 137,\n","        43, 141, 133,  39, 137, 126,  38, 130, 119,  35, 124, 114,  32,\n","       119, 108,  32, 114, 102,  31, 107,  95,  28,  99,  88,  25,  90,\n","        78,  25,  83,  77,  33, 104, 102,  84, 200, 200, 196, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 244, 244, 240, 185, 180,\n","       136, 182, 172, 106, 191, 179, 106, 192, 180,  99, 191, 180,  93,\n","       190, 177,  86, 186, 172,  79, 188, 173,  78, 187, 172,  77, 185,\n","       170,  71, 184, 170,  70, 184, 172,  72, 181, 169,  68, 176, 164,\n","        63, 171, 159,  59, 165, 153,  56, 163, 151,  55, 160, 150,  55,\n","       153, 144,  49, 150, 139,  46, 149, 138,  46, 146, 135,  41, 142,\n","       132,  38, 137, 127,  37, 130, 119,  34, 122, 109,  31, 113, 102,\n","        28, 108,  96,  26, 105,  92,  26, 101,  88,  31,  92,  82,  28,\n","        84,  76,  32,  98,  94,  68, 185, 185, 178, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 237,\n","       236, 230, 180, 176, 133, 176, 166, 106, 186, 174, 106, 189, 177,\n","       100, 186, 173,  89, 184, 169,  81, 182, 167,  77, 178, 163,  70,\n","       180, 164,  68, 179, 165,  67, 176, 164,  67, 174, 161,  64, 169,\n","       156,  59, 163, 150,  53, 157, 145,  48, 153, 140,  45, 149, 139,\n","        46, 143, 135,  41, 143, 133,  42, 144, 132,  42, 142, 128,  39,\n","       137, 126,  38, 130, 119,  37, 120, 110,  30, 112, 100,  26, 105,\n","        92,  25, 102,  90,  25, 101,  88,  29,  94,  83,  28,  84,  75,\n","        29,  96,  91,  63, 171, 171, 164, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 239, 239, 233, 178, 172, 136, 166, 155, 101, 179,\n","       165, 102, 179, 165,  90, 176, 160,  78, 175, 161,  77, 172, 159,\n","        72, 173, 161,  68, 173, 161,  64, 169, 156,  62, 168, 155,  61,\n","       162, 149,  55, 155, 142,  47, 149, 135,  43, 143, 130,  40, 137,\n","       127,  39, 134, 125,  38, 132, 123,  37, 134, 121,  37, 132, 119,\n","        37, 126, 114,  33, 117, 107,  29, 108,  99,  26, 101,  91,  23,\n","        99,  87,  26,  99,  86,  28,  93,  81,  28,  82,  71,  24, 104,\n","        99,  71, 186, 186, 179, 254, 254, 254, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 244, 243, 240, 184, 181, 154,\n","       155, 147,  98, 162, 152,  92, 164, 153,  82, 163, 152,  78, 163,\n","       152,  72, 163, 152,  69, 162, 152,  65, 162, 149,  60, 161, 147,\n","        58, 154, 141,  51, 146, 134,  44, 138, 124,  39, 129, 115,  36,\n","       123, 110,  34, 120, 107,  33, 117, 107,  33, 116, 106,  33, 113,\n","       102,  30, 107,  96,  24, 101,  89,  25,  97,  85,  27,  93,  83,\n","        25,  92,  81,  27,  85,  76,  25,  83,  77,  33, 141, 138, 117,\n","       223, 222, 219, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 254, 255,\n","       255, 212, 210, 194, 158, 154, 118, 144, 138,  91, 145, 135,  79,\n","       146, 136,  72, 146, 136,  67, 145, 136,  60, 148, 136,  57, 145,\n","       133,  53, 139, 128,  46, 130, 119,  39, 120, 106,  33, 110,  97,\n","        31, 105,  94,  32, 104,  94,  40, 111, 104,  54, 113, 106,  60,\n","       118, 110,  69, 118, 111,  73, 109, 102,  66, 105,  99,  61, 100,\n","        95,  53, 103,  99,  61, 140, 138, 114, 212, 211, 204, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 247, 247, 246, 215, 213, 203, 183, 179,\n","       158, 174, 171, 147, 168, 166, 138, 168, 163, 130, 159, 151, 112,\n","       150, 141,  95, 133, 125,  74, 127, 119,  64, 127, 114,  66, 117,\n","       107,  65,  99,  93,  61, 147, 145, 132, 227, 227, 224, 227, 227,\n","       225, 235, 235, 233, 239, 240, 240, 230, 230, 229, 231, 231, 227,\n","       228, 228, 223, 239, 239, 236, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255], dtype=uint8)"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":[""],"metadata":{"id":"dCItvZ_xjvzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"3weJfw_5kDob"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","machine_shape":"hm","name":"custom_cnn3.ipynb","provenance":[],"collapsed_sections":["lrVIgCDnsS3-","FE8mYSKMgRpP","DTG9gwi1eeT2","k9vL3DpOfUbw","Jl03f9MyzFQA","7H1wNq2pHGPD","KiQlOB7ut5pH","R9YYvxuWDMsg","0MfCJoZrMnzB","-1XEE0UlZl2C"],"authorship_tag":"ABX9TyP/D9kvty7tBiC47rgEEPfG"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}